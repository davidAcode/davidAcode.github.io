{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dave teaches how to create NN on Github 02142019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidAcode/davidAcode.github.io/blob/master/Dave_teaches_how_to_create_NN_on_Github_02142019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hD5zvcQ7gBLc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The best teacher is the person who just learned yesterday the stuff you are studying today, because she remembers what she struggled with and how she overcame it, and she can pass those shortcuts on to you.\n",
        "\n",
        "Today will be a giant step forward, both in your learning and your confidence in your mastery.  You are about to create your first neural network (NN) that will be able to guess, to learn from its mistakes, and to do better next time.  Your NN will solve a simple binary classifaction problem, i.e., \"given these 1's and 0's in the matrix X as inputs, what is your prediction of the 1's and 0's in matrix y will be?\"\n",
        "\n",
        "Thanks to super-teachers Andrew Trask and Siraj Raval, today's material--a working neural network in about 15 lines of Python--is the foundation of what you need to progress towards an understanding of the cutting edge techniques being used in deep learning today.  My mentor Adam Koenig, a Stanford PhD in aerospace engineering, helped me learn this stuff, and now I'm going to stand on these teachers' shoulders and help you learn it too.\n",
        "\n",
        "First read Andrew Trask's [blog post](http://iamtrask.github.io/2015/07/12/basic-python-network/) several times and absorb his teaching as best you can.  Then, return to this post.\n",
        "\n",
        "I suggest you open this blog post in two side-by-side windows and show the code in the left window while you scroll through my explanation of it in your right window.  First, I'll show you the entire code we'll be studying today, and below that is my detailed step-by-step explanation of what it does.  Get ready for the wonder of watching a computer learn from its mistakes and recognize patterns!  We're about to give birth to our own little baby brain... :-)\n",
        "\n",
        "Here's the code first.  Display this in your left window:\n"
      ]
    },
    {
      "metadata": {
        "id": "UohRpAqC_AuG",
        "colab_type": "code",
        "outputId": "403a889b-3382-455c-99ea-4ecb549d4a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3629
        }
      },
      "cell_type": "code",
      "source": [
        "#This is the \"3 Layer Network\" near the bottom of: \n",
        "#http://iamtrask.github.io/2015/07/12/basic-python-network/\n",
        "\n",
        "#First housekeeping: import numpy, a powerful library of math tools.\n",
        "import numpy as np\n",
        "#1 Sigmoid Function: changes numbers to probabilities and finds slope to use in gradient descent\n",
        "def nonlin(x,deriv=False):\n",
        "  if(deriv==True):\n",
        "    return x*(1-x)\n",
        "  \n",
        "  return 1/(1+np.exp(-x))\n",
        "#2 X Matrix: This is a set of inputs in the training set that we will use to \n",
        "#train our network.\n",
        "X = np.array([[0,0,1],\n",
        "              [0,1,1],\n",
        "              [1,0,1],\n",
        "              [1,1,1]])\n",
        "#3 y Vector: The output layer: Our training set of 4 target values. Once our NN\n",
        "#can correctly predict these 4 target values from the inputs provided by X, \n",
        "#it is now ready to predict in real life.\n",
        "y = np.array([[0],\n",
        "             [1],\n",
        "             [1],\n",
        "             [0]])\n",
        "#4 SEED: This is housekeeping. Ya gotta seed the random numbers we will generate\n",
        "#in the training process, to make debugging easier.\n",
        "np.random.seed(1)\n",
        "\n",
        "#5 SYNAPSES: aka \"Weights.\" These 2 matrices are the \"brain.\"  It remembers, learns, improves.\n",
        "syn0 = 2*np.random.random((3,4)) - 1 # First layer of weights, Synapse 0, connecting l0 to l1.\n",
        "syn1 = 2*np.random.random((4,1)) - 1 # Second layer of weights, Synapse 1 connecting l1 to l2.\n",
        "\n",
        "#6 FOR LOOP: this iterator takes our NN through 60,000 guesses, tweaks, and improvements.\n",
        "for j in range(60000):\n",
        "  \n",
        "  #7 FEED FORWARD NETWORK: Think of l0, l1 and l2 as 3 matrices as the \"neurons\" \n",
        "  #that combine with the \"synapses\" matrices in #5 to think, predict, improve, remember.\n",
        "  l0=X\n",
        "  l1=nonlin(np.dot(l0,syn0))\n",
        "  l2=nonlin(np.dot(l1,syn1))\n",
        "  \n",
        "  #8 TARGET, and how much we missed it by. y is a 4x1 vector containing our 4 target \n",
        "  #values. When we subtract the l2 vector (our first 4 guesses) from y, our target,\n",
        "  #we get l2_error: how much our neural network missed the target by on this iteration.\n",
        "  l2_error = y - l2\n",
        "  \n",
        "  #9 PRINT ERROR: in 60,000 iterations, j divided by 10,000 leaves a remainder of 0\n",
        "  #only 6 times. We're going to check our data every 10,000 iterations to see if\n",
        "  #the l2_error is reducing, and we're missing our target by less each time.\n",
        "  if (j% 10000)==0:\n",
        "    print(\"Our average l2 error after 10,000 more iterations: \" + str(np.mean(np.abs(l2_error))))\n",
        "\n",
        "  #10 In what DIRECTION is y, our desired target value, from our NN's latest guess? We\n",
        "  #take the slope of our latest guess, multiply it by how much that latest guess\n",
        "  #missed our target of y, and the resulting l2_delta tells us by what value to update\n",
        "  #each weight in our syn1 synapses so that our next prediction will be even better.\n",
        "  \n",
        "  #AK: The term \"direction\" seems misleading to me.  The delta modifies the error with weights \n",
        "  #from the derivatives to induce large changes in low confidence values and smalll\n",
        "  #changes in high confidence values\n",
        "  l2_delta = l2_error*nonlin(l2,deriv=True)\n",
        "  \n",
        "  #11 BACK PROPAGATION: After we \"fed forward\" in Step 7, no we work backwards to \n",
        "  #find the l1 error.  l1 error is the difference between the ideal l1 that would \n",
        "  #provide the ideal l2 we want and the most recent computed l1.  To find l1_error, \n",
        "  #we have to multiply how much our latest prediction missed the target (l2_error) \n",
        "  #by our last guess at the optimal weights (syn1). We'll then use l1_error to update syn0 below.\n",
        "  l1_error = l2_error.dot(syn1.T)\n",
        "\n",
        "  #12 In what DIRECTION is l1, the desired target value of our hard-working middle layer 1,\n",
        "  #from l1's latest guess?  Similar to #10 above, we want to tweak this middle layer\n",
        "  #so it sends a better prediction to l2, making it easier for l2 to better predict target y.\n",
        "  \n",
        "  #AK: I don't think l2 should be mentioned here.  It is similar to what was \n",
        "  #done before.  Add weights to produce large changes in low confidence values \n",
        "  #and small changes in high confidence values\n",
        "  l1_delta = l1_error * nonlin(l1,deriv=True)\n",
        "  \n",
        "  #13 UPDATE SYNAPSES: aka Gradient Descent. This step is where the synapses, the true\n",
        "  #\"brain\" of our network, learn from their mistakes, remember, and improve--learning!\n",
        "  syn1 += l1.T.dot(l2_delta)\n",
        "  syn0 += l0.T.dot(l1_delta)\n",
        "\n",
        "#Print results!\n",
        "print(\"Our l2 error value after all 60,000 iterations of training: \")\n",
        "print(l2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l0\n",
            "[[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [1 1 1]]\n",
            "syn0\n",
            "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
            " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
            " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
            "l1\n",
            "[[0.44856632 0.51939863 0.45968497 0.59156505]\n",
            " [0.28639589 0.32350963 0.31236398 0.51538526]\n",
            " [0.40795614 0.62674606 0.23841622 0.49377636]\n",
            " [0.25371248 0.42628115 0.14321233 0.41732254]]\n",
            "syn1\n",
            "[[-0.5910955 ]\n",
            " [ 0.75623487]\n",
            " [-0.94522481]\n",
            " [ 0.34093502]]\n",
            "l2\n",
            "[[0.47372957]\n",
            " [0.48895696]\n",
            " [0.54384086]\n",
            " [0.54470837]]\n",
            "l2_delta\n",
            "[[-7.51235207e-06]\n",
            " [ 7.55068179e-06]\n",
            " [ 6.74174863e-06]\n",
            " [-8.00425612e-06]]\n",
            "l1_delta\n",
            "[[ 9.46832171e-10 -3.08497572e-05  5.08949926e-05 -3.15810072e-05]\n",
            " [-5.49981023e-05  9.22271485e-11 -1.98772953e-05  4.06826431e-05]\n",
            " [-5.58922384e-05  4.01319985e-05 -2.09610752e-05  8.85971300e-11]\n",
            " [ 7.74711336e-05 -1.94497871e-05  8.78140259e-11 -1.97665444e-05]]\n",
            "Our average l2 error after 10,000 more iterations: 0.4964100319027255\n",
            "l0\n",
            "[[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [1 1 1]]\n",
            "syn0\n",
            "[[  8.64707399  10.00563742 -10.15630587 -10.65975519]\n",
            " [  8.42855584 -10.766145   -10.40147259   9.92746783]\n",
            " [-12.87422852  -5.1796668    4.71410902  -5.04790339]]\n",
            "l1\n",
            "[[2.56325896e-06 5.59836388e-03 9.91111855e-01 6.38179652e-03]\n",
            " [1.15932339e-02 1.18801486e-07 3.37707152e-03 9.92457005e-01]\n",
            " [1.43839408e-02 9.92045022e-01 4.31129282e-03 1.50747629e-07]\n",
            " [9.85246352e-01 2.62466374e-03 1.31577531e-07 3.07861912e-03]]\n",
            "syn1\n",
            "[[-5.06423978]\n",
            " [ 5.39200503]\n",
            " [-5.19713657]\n",
            " [ 5.18361605]]\n",
            "l2\n",
            "[[0.0061344 ]\n",
            " [0.99374596]\n",
            " [0.99479989]\n",
            " [0.00696814]]\n",
            "l2_delta\n",
            "[[-3.74032096e-05]\n",
            " [ 3.88717977e-05]\n",
            " [ 2.69023788e-05]\n",
            " [-4.82216494e-05]]\n",
            "l1_delta\n",
            "[[ 7.96506370e-08 -1.84156933e-04  2.80872472e-04 -2.01656726e-04]\n",
            " [-3.62962127e-04  4.00699665e-09 -1.09404489e-04  2.42712051e-04]\n",
            " [-3.73386587e-04  2.21295411e-04 -1.16023443e-04  4.06430258e-09]\n",
            " [ 5.13005507e-04 -9.83639169e-05  4.76602221e-09 -1.10868225e-04]]\n",
            "Our average l2 error after 10,000 more iterations: 0.006139171883586087\n",
            "l0\n",
            "[[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [1 1 1]]\n",
            "syn0\n",
            "[[  9.59707531  10.86409702 -10.98766687 -11.44493829]\n",
            " [  9.44566442 -11.48394354 -11.1804635   10.83239698]\n",
            " [-14.38564356  -5.60387659   5.10874187  -5.52224074]]\n",
            "l1\n",
            "[[5.65449910e-07 3.67003758e-03 9.93992625e-01 3.98097046e-03]\n",
            " [7.10392091e-03 3.79187484e-08 2.30188674e-03 9.95083140e-01]\n",
            " [8.25564427e-03 9.94832680e-01 2.78998549e-03 4.27806872e-08]\n",
            " [9.90595297e-01 1.97793284e-03 3.90123123e-08 2.16151504e-03]]\n",
            "syn1\n",
            "[[-5.38758924]\n",
            " [ 5.59484942]\n",
            " [-5.46579488]\n",
            " [ 5.45926967]]\n",
            "l2\n",
            "[[0.00453811]\n",
            " [0.99542049]\n",
            " [0.99595496]\n",
            " [0.00489782]]\n",
            "l2_delta\n",
            "[[-2.05018863e-05]\n",
            " [ 2.08768375e-05]\n",
            " [ 1.62967363e-05]\n",
            " [-2.38723922e-05]]\n",
            "l1_delta\n",
            "[[ 1.38266638e-08 -9.28450293e-05  1.48121037e-04 -9.82400509e-05]\n",
            " [-1.74035762e-04  9.71643617e-10 -5.74878643e-05  1.22327060e-04]\n",
            " [-1.78439310e-04  1.16344705e-04 -6.15155299e-05  9.44823661e-10]\n",
            " [ 2.45845349e-04 -5.40956950e-05  1.04449554e-09 -5.76736025e-05]]\n",
            "Our average l2 error after 10,000 more iterations: 0.004515120400193422\n",
            "l0\n",
            "[[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [1 1 1]]\n",
            "syn0\n",
            "[[ 10.13918414  11.36943321 -11.49094318 -11.9162404 ]\n",
            " [ 10.0210231  -11.93134258 -11.65128802  11.35377575]\n",
            " [-15.23982278  -5.85192856   5.34722117  -5.79192828]]\n",
            "l1\n",
            "[[2.40673879e-07 2.86611224e-03 9.95261199e-01 3.04280431e-03]\n",
            " [5.38467188e-03 1.89157457e-08 1.82551337e-03 9.96173030e-01]\n",
            " [6.05595617e-03 9.96000206e-01 2.14231849e-03 2.03910716e-08]\n",
            " [9.92756526e-01 1.63604209e-03 1.86949939e-08 1.73607144e-03]]\n",
            "syn1\n",
            "[[-5.57750745]\n",
            " [ 5.73200487]\n",
            " [-5.63406508]\n",
            " [ 5.62991048]]\n",
            "l2\n",
            "[[0.00378164]\n",
            " [0.99619659]\n",
            " [0.99654073]\n",
            " [0.003998  ]]\n",
            "l2_delta\n",
            "[[-1.42471593e-05]\n",
            " [ 1.44113390e-05]\n",
            " [ 1.19255001e-05]\n",
            " [-1.59206199e-05]]\n",
            "l1_delta\n",
            "[[ 5.07673875e-09 -6.19509560e-05  1.00489726e-04 -6.45873230e-05]\n",
            " [-1.13616844e-04  4.12414455e-10 -3.90481306e-05  8.16354591e-05]\n",
            " [-1.16140899e-04  7.89955444e-05 -4.16652180e-05  3.97152856e-10]\n",
            " [ 1.60356934e-04 -3.74322521e-05  4.21136081e-10 -3.90095728e-05]]\n",
            "Our average l2 error after 10,000 more iterations: 0.0037605808057822217\n",
            "l0\n",
            "[[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [1 1 1]]\n",
            "syn0\n",
            "[[ 10.5187793   11.72800065 -11.85158643 -12.25416383]\n",
            " [ 10.42107334 -12.25768923 -11.98994816  11.7202093 ]\n",
            " [-15.83390572  -6.02781963   5.51872336  -5.98025135]]\n",
            "l1\n",
            "[[1.32868509e-07 2.40494685e-03 9.96005058e-01 2.52181511e-03]\n",
            " [4.43919843e-03 1.14473351e-08 1.54493888e-03 9.96795399e-01]\n",
            " [4.89260191e-03 9.96665794e-01 1.77378707e-03 1.20474214e-08]\n",
            " [9.93975912e-01 1.41740631e-03 1.10281887e-08 1.48003874e-03]]\n",
            "syn1\n",
            "[[-5.71291646]\n",
            " [ 5.83703394]\n",
            " [-5.75762397]\n",
            " [ 5.75464246]]\n",
            "l2\n",
            "[[0.00331468]\n",
            " [0.99667165]\n",
            " [0.99691898]\n",
            " [0.00346452]]\n",
            "l2_delta\n",
            "[[-1.09509489e-05]\n",
            " [ 1.10412872e-05]\n",
            " [ 9.46363958e-06]\n",
            " [-1.19616060e-05]]\n",
            "l1_delta\n",
            "[[ 2.51621692e-09 -4.64199586e-05  7.59395055e-05 -4.79830721e-05]\n",
            " [-8.40369607e-05  2.22407172e-10 -2.95612253e-05  6.11840168e-05]\n",
            " [-8.56986331e-05  5.97639750e-05 -3.14108161e-05  2.13614458e-10]\n",
            " [ 1.18516657e-04 -2.86235682e-05  2.19996116e-10 -2.94646841e-05]]\n",
            "Our average l2 error after 10,000 more iterations: 0.003297142668497631\n",
            "l0\n",
            "[[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [1 1 1]]\n",
            "syn0\n",
            "[[ 10.81057008  12.00571899 -12.13217915 -12.51769832]\n",
            " [ 10.72689601 -12.51471549 -12.25453794  12.002469  ]\n",
            " [-16.28829765  -6.16414066   5.65267007  -6.12473726]]\n",
            "l1\n",
            "[[8.43494347e-08 2.09910970e-03 9.96504127e-01 2.18328875e-03]\n",
            " [3.82867055e-03 7.72459715e-09 1.35598821e-03 9.97206693e-01]\n",
            " [4.16142516e-03 9.97104155e-01 1.53221256e-03 8.01111740e-09]\n",
            " [9.94775554e-01 1.26282905e-03 7.30982112e-09 1.30536473e-03]]\n",
            "syn1\n",
            "[[-5.81838246]\n",
            " [ 5.92254494]\n",
            " [-5.85554369]\n",
            " [ 5.85325429]]\n",
            "l2\n",
            "[[0.00298888]\n",
            " [0.99700164]\n",
            " [0.99719127]\n",
            " [0.00310119]]\n",
            "l2_delta\n",
            "[[-8.90688026e-06]\n",
            " [ 8.96336198e-06]\n",
            " [ 7.86694840e-06]\n",
            " [-9.58771502e-06]]\n",
            "l1_delta\n",
            "[[ 1.46694588e-09 -3.70807433e-05  6.09704438e-05 -3.81133452e-05]\n",
            " [-6.65390873e-05  1.37178652e-10 -2.37752892e-05  4.88870129e-05]\n",
            " [-6.77255795e-05  4.80333634e-05 -2.51616578e-05  1.31710265e-10]\n",
            " [ 9.37787428e-05 -2.31654184e-05  1.32745977e-10 -2.36645503e-05]]\n",
            "Our average l2 error after 10,000 more iterations: 0.002974289227268084\n",
            "Our l2 error value after all 60,000 iterations of training: \n",
            "[[0.00274464]\n",
            " [0.99724836]\n",
            " [0.99740013]\n",
            " [0.0028332 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w6zpHJcEWoCQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's go through each step of the code in detail, beginning with\n",
        "lines 6-11:\n",
        "\n",
        "\"nonlin\" above is a type of standard logistic function known as a Sigmoid function.  Logistic functions are very commonly used in science, statistics, and probability.  This Sigmoid function is written in a more complicated way than necessary because it serves two functions:\n",
        "\n",
        "1) to take each of the matrices within its parentheses and convert each value to a number between 0 and 1 (aka a statistical probability).  This is done by line 11: `return 1/(1+np.exp(-x))` \n",
        "We will see below that this is very important, because this conversion to a 0-1 number gives us **FOUR** very **big advantages**.  I will discuss these four in detail below, but for now, just know that the sigmoid function converts every number in every matrix within its parentheses into a number between 0 and 1 that falls somewhere on the S-curve illustrated here:\n",
        "\n",
        "![alt text](https://iamtrask.github.io/img/sigmoid.png)\n",
        "(taken with gratitude from: \n",
        "[Andrew Trask](https://iamtrask.github.io/2015/07/12/basic-python-network/))\n",
        "\n",
        "So, Part 1 of the Sigmoid function has converted a number into a number between 0 and 1, i.e., a statistical probability, which is also known as a confidence measure.  In other words, the number answers the question, \"how confident are we that this number correctly predicts an outcome?\"  \n",
        "\n",
        "Now for Part 2.  The second part of this sigmoid function is in lines 8 and 9:\n",
        "'  if(deriv==True):\n",
        "    return x*(1-x)'\n",
        "When called to do so by `deriv=True` in the code below, line 9 takes the confidence measure from Part 1) and converts it into a slope of the Sigmoid S curve, which will be used to tweak the synapse matrices of our NN and nudge them towards greater accuracy in prediction.  \n",
        "\n",
        "So the sigmoid function plays a super-important role in making our NN learn, but don't worry if you don't understand it all yet.  I'll explain it in detail below.  Let's move on to Step 2:\n"
      ]
    },
    {
      "metadata": {
        "id": "wIaXZ66kngxi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#2 X Matrix: The input. Our training set of 4 initial \"guesses\" that we'll use to\n",
        "#make better-and-better predictions that will eventually attain our target, correct answers.\n",
        "X = np.array([[0,0,1],\n",
        "              [0,1,1],\n",
        "              [1,0,1],\n",
        "              [1,1,1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WwxKZKxqwP0y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2) Creating X input: Lines 12-17\n",
        "Lines 12-17, step 2, create a 4x3 matrix of input values that we will use to train our network.  X will become layer 0, or l0 of our network, so this is the beginning of the \"toy brain\" we are creating!  \n",
        "\n",
        "Think of each row of this matrix as a training example we'll feed into our network, and each column is one node of our input.  So our Matrix X can be visualized as this:\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/1cxj3K8bj_IOL8lhF9pRQSbDQyo3imZGVmZlEsjfvX1ImXBIJZHeJgOtX2kOQoaEbfhCbqtR2JFzSXK_r41dGGVdH6rde7QgoIqEGGn-v37CcD6IWw-nk9put1txPGeKvP-fxo3Lwk3nlwd3nkIdgjMu8HQf9ccyAZGlgRj7YH3avElDzwJr6kEDygOaLxmR9xo427aQHMwSjhdQKrrHwfTSsrUZmXyKh5F2anucPUAB-BAfTNyIG4brluYZwwKq3Mhx4eOJkn51V0D4zlcDVDPEK7k789ZUYVSfEcJ4_QbIY7YVeSHkqO-jQ3XB6XeK1WydnTFF8-OcOgBEzuKB6LDUNUYttH8eKE-k7OB5vi7h56PmIK35-ciOix52zaQRP0s8kcBeL5NAimjhIZjC6j5Am7CAf7VeBZUs9q9UvemlNYRZze_SWmlJIQBVx05GOShiGlHXAYZxdEWSOT9P4B09RmeqVCxakgQbgzOpS-05KVFzcIDn8HYsKOLj05uURaLfPL8AP3kKEOSE4z3iSdBxVnRuu6taQcRBU9q9UbZbdo7z0vXPbacLuxc1nPlvOkb_0fcHQxqU1Dn-i17fBUeV1Mcv_sjxhvXzvlVJq3qhJAyYD5YYDi0AgLLIU-o2XZ31DGFkvNQxioM8mPT5tOeycr-n4sfS=w518-h389-no\n",
        ")\n",
        "\n",
        "Notice that I have turned the 4 rows (aka 4 training examples) onto their sides to make 4 columns.  I don't mean to confuse you. It is very important to visualize matrices as nodes, and you will see why below.  But first, we have to do some housekeeping and create the other parts of our NN.  Next: creating our output layer, also known as our target values:"
      ]
    },
    {
      "metadata": {
        "id": "ze1bWVq0yJGD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#3) Create y output: Lines 18-24\n",
        "This code creates our output layer: Our training set of 4 target values. Once our NN can correctly predict these 4 target values from the inputs provided by matrix X above, it is now ready to predict in real life.  Think of X, the input layer, layer 0, as the beginning of our NN.  y, the output layer, is the end:\n",
        "![alt text](https://lh3.googleusercontent.com/6J6pt5FDEceBFHqP18HZ3rdWE3oRHhOlFG2ZAeng6vROeQZuLajnYrCw2oAmPv39yYnDHi2R1K91kF1B9fyAZePNaTcZhyxOiW7U1MPeQUyBcMWH02wT4yrVyWIHn9Yz8vXlASd9G14mVgoxTvRvWtDBsQlVwf2gn48TLy3Xm7Z7JepB5X0yk_xusS5-7nwtspG8NkKQwVrJjCEzwR6wAN5KgUfFd15r1RI-6Hwb082hcipNWl5TX8pqvNdrMkrOjYHnNexc-CUGwIESRd3ramL72W4Wb1B8WciircrSFlt3bHRNO_11ZVB5W061dOhV8O9xhmPRiFQQImS99UpbK7Lwuu5FG4hLyr6rYPJsyMsMlT5yOsSjlypBTrKEJq8USJJmsPrGdbQ_hKtwuo-ywnmOTS4Bpz0eSI_JxNfzV4hFN4uo9LxcLqlSvUBH72HlDlPSsMpyMroieRJifYraW6XXpeZ__DXSbxWkm9oz4x-W1YJNQV_pljbHEEHTv4iGS22YyyLd84gPYoagm74ptwXodx2jJi9KHy2E_2HBvpVmQVgVgNFZq7-JO13PzzSOhhpT57EzBr1ToMbjRAaNl4KHCBjJY-5-l7mE1Q5-NTtMH5MV219sPi4za2Fq06hfDmJKMKDei1BgFxkczdmr92PdCm4-98z9=w518-h388-no)\n",
        "\n",
        "Adam, am I correct that the above is only 1 node?\n"
      ]
    },
    {
      "metadata": {
        "id": "2KbWE54s1ETU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#4) Seed your random numbers: Lines 25-27\n",
        "This step is housekeeping. We have to seed the random numbers we will generate in synapses/weights for the next step in our training process, to make debugging easier.  You don't have to understand how this codes works, you just have to include it."
      ]
    },
    {
      "metadata": {
        "id": "LOXeombt1uOL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#5) Create \"Synapses\" of your brain--Weights: Lines 29-31\n",
        "These 2 matrices are the \"brain\" of our NN.  These layers are the part of our NN that learn by trial-and-error making predictions, then improve their next prediction, then remember their improvements--learning!\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/S-ihGzoU2SJQvm36_2f0ORWDmJFe-gs2tjVB-MEZgKXAfhpLzwroAg7x7w7ynCDbycJtytdC5KmRTj0yNQ0mFBYLgE26L7CTl8HeZfjXnqOQkPqyR0tWRSmKg2VxscEEwmhEAu06kmI4YddkLXVIqx0cjPcuLqQSdOfprAUbH661aB5-H5XPdkiW83RCRGUYvUYZM0rpQhJM3LuV8jAO9ZEO3il87MQEahMKOPrsNp2KWsbDX9HF3B17_cz_ZyYRa7to30FdKOaOxL1R8ccFOzGHfEtfv5B5AuBX8Nqxq6LV-j1DSU_OTosxenEY238Bpc8aUnTLniN6T65YLQ1Re17yK28z3B-Mcc4OHsRYHObk2pZU7ASedCSO8SgW4pUyT1IJZpei0wNGLYRi50kLCgDraQr07aDodr-HpkP7jRAQcEFNWnFGPNsvA3sQz4QT2wZhEx9kVJLZdVHEsglCE1_bzdzKnE3MPv0QNAz72juBmicCxcNEv_p-BAlQsvkDrES1QdIn0PTXJAo8Wd2f1RndNWqdxlA4qBU1cwhNPjouv1CFbAQnDL9IiwbeXFD5AUjL4jDDUZYt8MGwZ1Mg6YGakLOWz-RZ05uitJFtQSV-UgC4tZ6K02kbLbv-8JYDK5-JoRfap6S6JeTapIa_nf_f1IFg397-=w968-h238-no)\n",
        "\n",
        "Notice how this code, `syn0 = 2*np.random.random((3,4)) - 1` creates a 3x4 matrix and seeds it with random values.  This will be the first layer of weights, Synapse 0, that connects l0 to l1.  You'll visualize it below in a minute.\n",
        "\n",
        "The function np.random.random produces random numbers uniformly distributed between 0 and 1 (with a corresponding mean of 0.5).  But we want this initialization to have a mean zero.  Why?  So that the initial weight numbers in this matrix do not have an a-priori bias towards values of 1 or 0, because this would imply a confidence that we do not yet have (i.e. in the beginning, the network has no idea what is going on so it should display no confidence until we update it after each iteration).  \n",
        "\n",
        "So, how do we convert a set of numbers with an average of 0.5 to a set with a mean of 0?  We first double all the random numbers (resulting in a distribution between 0 and 2 with mean 1) and then we subtract one (resulting in a distribution between -1 and 1 with mean 0).  That's why you see 2* at the beginning of our equation, and - 1 at the end: `2*np.random.random((3,4)) - 1`\n",
        "\n",
        "Notice that we are generating a 3x4 matrix.  Why?  Because l0 (aka our X matrix) is a 4x3, and matrix multiplication requires the inner 2 size numbers to match, i.e., a 4x3 matrix must be multiplied by a 3x_?_ matrix--in this case, a 3x4.  See how those inner two numbers must be the same?\n",
        "\n",
        "Then this line of code, `syn1 = 2*np.random.random((4,1)) - 1` creates a 4x1 vector and seeds it with random values.  This will be our NN's second layer of weights, Synapse 1, connecting l1 to l2.  Keep an eye on the size of each matrix we are creating, because this will become *very* important soon.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3HiGfaOK9-ST",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#6) For Loop: Lines 33-34\n",
        "This is a for loop that will takes our NN through 60,000 iterations.  For each iteration, our network will take X, our input data, and based on that data, give its best guess at a prediction of what our y output is. It will then analyze how it did, learn from its mistakes, and give a slightly better prediction on the next iteration.  60,000 times, until it has learned by trial-and-error how to take the X input and predict accurately what the y output is.  Then our NN will be ready to take *any* input data you give it and correctly predict its future!\n"
      ]
    },
    {
      "metadata": {
        "id": "kCds2hNH_etM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#7) Feed Forward Network: Lines 36-40\n",
        "This is where our NN makes its first guess at a prediction. Think of l0, l1 and l2 as 3 matrices as the \"neurons\" that combine with the \"synapses\" matrices we created in #5 to think, predict, improve, remember.  This is where matrix multiplication becomes key.\n",
        "\n",
        "First, we take the dot product of the 4x3 l0 and the 3x4 Syn0 to create (hidden layer) l1, a 4x4:\n",
        "![alt text](https://lh3.googleusercontent.com/Qbej-Y21C2su8KnB4dzus7l39oJ8c1VZ-3oLOJLqWrb6sr_pAjyq2vBPJ8Y8VG8JPaQQKOybzyTo7sqgDI6OPdd24VAR71aMXSzXRpaPpxLsYwgvEhWJxC0CwtUIlJp5Xjar-jvUMBrybLnnsbhJ9s-cf1Exci0nlUNFCiFjtH1O5SdST71fPXhZoUUOxN9-hWv6Msr5Obpg0ve5RrZc_VvSGD9I85nqxRvpaILZkQQLeggj4tlBW528lymNR9e28gwOy9fuAopRaM37Mfw0or9aIZiK4jjiSuU0FoHX757Mv0JgvmM6QgVOb2sNxpX6Lx2eASFX6nxctGwXW0hX1wnzZRRf_KttjjeOTSa6r70KZdKy3f8RH4JK3oxroGBI4RXWOaSwNHHgmd7ljoo0uvUmKy3ScE0cMummCWfTNAv8dcvvi53JqbixVglUL_DQ-v-P4ZyW5HY2xJTWZ8KgByme0FlZTQFURPkvHFw7g375kI3Ad188o5L9p30SlvyKZ_N5Dp2VXHS5cavs42-OVWY47v0fYijR8VpPBOAfWNrcVAn1jOO_YLOwv7z3VFc8RbyMSoAFve0b3m6qGNHh-oYvpVhAuelaUzRWb5x0HYSLUq-wQZRksy8klBo2LWhQxsYTFoZSKLocyNj83uBnCd5-lrAve-SU=w968-h594-no)\n",
        "\n",
        "\n",
        "Notice that line 39 uses the `np.dot()` syntax because we are taking the dot product of two matrices.  Later we'll use the simpler multiplication syntax, i.e.,`(*)` when we are simply multiplying two vectors.  But you must be clear on the size of matrices you are multiplying in order to use the right syntax.\n",
        "\n",
        "Still on line 39, we next take the Sigmoid function of l1 because l1 may have values between 1 and -1, and we need it to have values between 0 and 1, hence: `l1=nonlin(np.dot(l0,syn0))`\n",
        "\n",
        "It is on line 39 that we see ***Big Advantage #1*** of the ***Four Big Advantages of the Sigmoid Function.***  When we pass the dot product matrix of l0 and syn0 through the `nonlin()` function, the sigmoid converts each value in the matrix into a statistical probability between 0 and 1.  This means, \"the closer the value is to 1, the more certainty there is that such-and-such is the case, whereas the closer the value is to 0, the more certainty that such-and-such is NOT the case.  \"So, what?\" you may ask.  Well, it doesn't matter in lines 39 and 40, but it matters a *ton* when we hit line 61 and beyond.  Stay tuned.\n",
        "\n",
        "Exactly the same thing happens on line 40, as we take the dot product of 4x4 l1 and 4x1 syn1 and then run that product through the Sigmoid function to produce a 4x1 l2 with each value becoming a statistical probablility from 0-1.\n",
        "\n",
        "Now we have completed the Feed Forward portion of our network.  This would be a good time to visualize what we have done so far, both in terms of the matrices involved, and also the layers of \"neurons\" and the \"synapses\" connecting those neurons:\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/Rsh6bYtBEu_oAMLVXppmxO0ayEBDJVQg8YwDZGz0_I6JKB588v5aVQbzQKrx-srfY1sGZm4V83NuSIPxaWBntZH3Hm584xbax4RR0f3L5IAOLSB3jH5ZNXJnnhl7gAQR9w2UsqOuxsdoRDXj1xrmLZkC_ZCEZrooNcHJRpEPySTsTiTb61j9AQg-uH2TLUewt02YPshI4i0f3Ynoc9IHa6IgywBH_k_Baae6FcOIuTPApdSiLDviMoZuIZLMOdGusJ1oV_E4sl7wWHcr4JQOlpz8rGArnyLkUywKh_1BJigH_z8s1EcUuA6NfUY8W_4qGJsEqLKDac2hhvvgqoUQTiLqis7PKp_8eS-MjpSv5tz17gclQAj4snNmlF_gEKLWMgRuPVju1zGmoMZeMPqMnXDE-QWVVAIKFYwtCe83KELSDKmkRCI_hySi0MYualR9tL7EPRdSjpS6VWoChupgnHodSdPKmNkx7qcGI8vE-BMWiVzxC1gxICM9DUc5h7LCAkwR2p4tN94tQB8z12AxI5-TI8l_tT1-RvgrhqO5QJv2tBzud7P6rJBUZxSVHqCt4wlP4ZbVgAviCwJDpzLzbsI3HE9666hHMNZLcUqsTd2YRwaV-p7m8DTcDmy4IzB36qNMDSNip8VG4CunowqCbQlw6kXBzVYp=w968-h698-no)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jHKp8pPNiiX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#8) By How Much Did We Miss the Target? Lines 42-45\n",
        "The 4x1 y vector is our goal, our target.  Given our input X of layer 0, we want to produce an output, layer 2, that is as close to the 4 values of y as possible.  Each one of our 60,000 iterations should bring us, by trial-and-error and learning from our mistakes, closer to the 4 target values of y.  So, for each iteration, we take our best prediction so far, the 4x1 vector l2, and subtract it from the 4x1 vector y.  The remainder is l2_error, i.e., how much each value of l2 missed its target value in y.  \n",
        "\n",
        "This is the exciting first step in the learning process of our NN.  Once we know what we missed by, in the following steps we will seek to correct that error and do better next time."
      ]
    },
    {
      "metadata": {
        "id": "oU2-ZoUJlQWd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#9) Print Error: Lines 47-51\n",
        "Line 50 is a clever parlor trick to have the computer print out our l2_error every 10,000 iterations.  The line, `if (j% 10000)==0:` means, \"If your iterator is at a number of iterations that, when divided by 10,000, there is no remainder, then...\"  ` j%10000 `would have a remainder of 0 only six times: at 0 iterations, 10,000, 20,000, and so on to 60,000.  So this print-out gives us a nice report on the progress of our NN's learning.\n",
        "\n",
        "The code `+ str(np.mean(np.abs(l2_error))))` simplifes our print out by taking the absolute value of each of the 4 values, then averaging all 4 into one mean number and printing that."
      ]
    },
    {
      "metadata": {
        "id": "C810qURdm3hZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#10) In What DIRECTION is y?\n",
        "You might call Step 10, \"How much do I tweak my NN before its next iteration and prediction?\"  For statistics and calculus buffs, we could simply say, \"In line 61 we compute how much the l2_delta needs to modify the error with weights from the derivatives to induce large changes in low confidence values and smalll changes in high confidence values.\"  Whew!  For the rest of us mere mortals, let me unpack that a bit:\n",
        "\n",
        "Here is where you will see the beauty of the Sigmoid function in four *magical* steps.  To me, the genius of neural networks' ability to think is found largely in these four steps.  We saw how, in line 39, Step 1 was then the `nonlin()` transformed each value of our matrix into a statistical probability between 0 and 1.  But I have yet to mention that that statistical probability is ***also*** a simple measure of confidence--numbers approaching 1 suggest high confidence that the NN's (neural network's) prediction is correct.  Numbers approaching 0 suggest high confidence that the NN's prediction is incorrect.  But what matters here is ***confidence.***  If our NN's prediction, the l2 value, is high-confidence and high-accuracy, that's an oustanding prediction, and we want to leave the syn0 and syn1 weights that produced that oustanding prediction alone.  We don't want to mess with what's working; we want to fix what's NOT working.\n",
        "\n",
        "That's why we focus our attention on the numbers in the middle:  all numbers approaching 0.5 in the middle are wishy-washy, and lacking confidence.  So, how can we tweak our NN to produce four l2 values that are both high-confidence and high-accuracy?  \n",
        "\n",
        "The key lies in the values, or ***weights*** of syn0 and syn1.  As I mentioned above, syn0 and syn1 are the center, the absolute *brains* or our neural network.  We are going to take the four values of the l2_error and perform beautiful, elegant math on them to produce an l2_delta.  l2_delta means, basically, \"the amount by which we need to increase-or-decrease our weight values in syn0 and syn1 in order to reduce wishy-washyness and maximize the high-confidence, high-accuracy values of the prediction of our next iteration.\"\n",
        "\n",
        "***Get ready for beauty.***\n",
        "\n",
        "Here is ***Big Advantage #3*** of the ***Four Big Advantages of the Sigmoid Function:*** Do you remember that diagram of the beautiful S-curve of the Sigmoid function that I showed you above?  Well, lo-and-behold, each of the 4 probability/confidence numbers of l2 lies somewhere on the S curve of the sigmoid graph (pictured again below, but this time with more detail).  If we search for that number (e.g. 0.9) on the Y axis of the graph below, we can see that it falls on the S curve roughly where you see the green dot: ![alt text](https://iamtrask.github.io/img/sigmoid-deriv-2.png)\n",
        "(taken with gratitude from: \n",
        "[Andrew Trask](https://iamtrask.github.io/2015/07/12/basic-python-network/))\n",
        "\n",
        "Note that the S curve above has very shallow slope at both the upper extreme (near 1) and the lower extreme (near 0).  Does that sound familiar?  Wonder of wonders, a shallow slope on the sigmoid curve coincides with high confidence and high accuracy in our predictions!  This means that a shallow slope equals a tiny number.  \n",
        "\n",
        "Therefore, when we go to update our synapses, we basically want to leave our high confidence weights alone because they already have good accuracy.  And, miracle-of-miracles, because our high-confidence/low-slope numbers end up being so tiny, multiplying the values of syn0 and syn1 by these teeny-tiny numbers has exactly the effect we want: our confident, accurate, high-performing values are largely left alone, and unchanged;\n",
        "\n",
        "The great news is that our wishy-washy indecisive weights, those in the middle of the S-curve, are the numbers that leave the biggest \"footprint\" on our S-curve.  What I mean is, the values around 0.5 can be traced on the Y axis of our graph below to the middle of the S-curve, where the slope is steepest, and therefore the value of that slope is a big number.  \n",
        "\n",
        "When we update our synapse matrix by multiplying the corresponding element with that large slope number, it's going to give that element a big nudge in the right direction towards confident and accurate prediction.  When I say, \"in the right direction,\" what I mean is that some values of our l2_delta are going to be negative, because we want them to reduce the weight values closer to 0 in syn0 and syn1.  Other values of our l2_delta are going to be positive, because we want them to increase the weight values closer to 1 in syn0 and syn1.  \n",
        "\n",
        "So it's important to notice that there is a sense of \"direction\" involved here.  Have you heard of gradient descent described as, \"a ball dropped in a bowl and rolling back-and-forth until it comes to a rest at the global minimum, the bottom of the bowl?\"  That's what the Sigmoid does for us.  It helps us to find the bottom of the bowl, the minimum of the cost function, the lowest error in our predictions.  Think of our 60,000 iterations as the ball rolling back-and-forth in the bowl until it no longer needs to change direction because it has come to rest at the ideal, perfect bottom of that bowl.  Picture it like this:\n",
        "\n",
        "![alt text](https://mail.google.com/mail/u/0?ui=2&ik=e3f869f938&attid=0.2&permmsgid=msg-a:r4352876950048414936&th=1691255aa52a4d54&view=fimg&sz=s0-l75-ft&attbid=ANGjdJ8FdFORGv3w0jn-Bs8GhlKpg2D1XPRzSF6OaNCqE8hchNYMIAymIg-nK1xCdIsQup54rJmkW2l0qttCzg03Hq8PJOv4KX0ae14e2dkswvLMt74Rzdhwt2ZJQBQ&disp=emb&realattid=ii_jsexnu8o2)\n",
        "\n",
        "START HERE with text explaining the above and transitioning into the one below. Give credits to Grant Sanderson for both!\n",
        "\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/jIup60T65tIKtXg0B-Np6jeNXk4TvQTRgBI1btNRZUZ4yy_ZEyL1bN3RwiSjzKNcbyXQN6z7vdV55NzGFxJfUpZXkyU6HTmrScht0rbk5BXGC6eO79LrZuuVpJdHE4fr4QYwvdbO)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Take your time with the above points and make sure you understand them.  Do you see why the sigmoid function is a thing of beauty?  It takes any random number in one of our matrices and turns it into a statistical probability, which turns into a confidence level, which turns into a big-or-small tweak of our synapses, always in the direction of greater confidence and accuracy.  The sigmoid function is the miracle by which mere numbers in a matrix can \"learn.\" A single number, along with its many colleagues in a matrix, can indicate probability and confidence, and that matrix can learn and remember that learning, over-and-over again as it improves with each iteration!  \n",
        "\n",
        "\n",
        "In Step 8, when we subtracted \n",
        "In what DIRECTION is y, our desired target value, from our NN's latest guess? We\n",
        "  #take the slope of our latest guess, multiply it by how much that latest guess\n",
        "  #missed our target of y, and the resulting l2_delta tells us by what value to update\n",
        "  #each weight in our syn1 synapses so that our next prediction will be even better.\n",
        "  \n",
        "  #AK: The term \"direction\" seems misleading to me.  \n",
        "  l2_delta = l2_error*nonlin(l2,deriv=True)\n",
        "b) Each of these statistical probabilities is also a simple measure of confidence\n",
        "\n",
        "c) Each of these confidence numbers \n"
      ]
    },
    {
      "metadata": {
        "id": "h3UuZItMpskW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(Adam, as you can see, I have several chunks of code to explain before I get to the code where we're finding l2_delta and updating the synapses and such, but I'm very proud of my explanation of the Sigmoid function and why it matters so much.  Can you please edit any mistakes or ambiguities?  If Colab will allow you to SUGGEST EDITS, please do that.  If not, then please edit in ALL CAPS and if something needs deleting, just write DELETE THIS UP TO... in all caps. Thanks, Dave)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LbnN5_91IXvp",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "*teach my Sigmund Freud ditty\n",
        "*diagram showing matrices, etc.\n",
        "*the best teacher  etc...\n",
        "import numpy as np\n",
        "#create sigmoid function to change numbers to probabilities\n",
        "def nonlin(x,deriv=False):\n",
        "  if(deriv==True):\n",
        "    return x*(1-x)\n",
        "  \n",
        "  return 1/(1+np.exp(-x))\n",
        "# neural layers input: each row is a training example, each column is a layer of nodes\n",
        "X = np.array([[0,0,1],\n",
        "              [0,1,1],\n",
        "              [1,0,1],\n",
        "              [1,1,1]])\n",
        "# output layer: 4 target values (training examples). Final Layer of the Network,\n",
        "# which is our hypothesis, and should approximate the correct answer as we train\n",
        "y = np.array([[0],\n",
        "             [1],\n",
        "             [1],\n",
        "             [0]])\n",
        "# seed your random numbers to make debugging easier\n",
        "np.random.seed(1)\n",
        "\n",
        "# randomly initialize your weights with mean of 0\n",
        "syn0 = 2*np.random.random((3,4)) - 1 # First layer of weights, Synapse 0, connecting l0 to l1.\n",
        "syn1 = 2*np.random.random((4,1)) - 1 # Second layer of weights, Synapse 1 connecting l1 to l2.\n",
        "\n",
        "for j in range(60000):\n",
        "  \n",
        "  # Feed forward through layers 0, 1, and 2\n",
        "  l0=X\n",
        "  l1=nonlin(np.dot(l0,syn0))\n",
        "  l2=nonlin(np.dot(l1,syn1))\n",
        "  \n",
        "  #Purpose of the line below is to determine by how much did the neural \n",
        "  #network miss in its prediction of the target value? (see notebook drawing) l2 \n",
        "  #is a 4x1 vector containing the NN's best guess of what the value is in the \n",
        "  #vector y.  y is a 4x1 vector of test data. We do element-wise subtraction\n",
        "  #of l2 from y.\n",
        "  l2_error = y - l2\n",
        "  \n",
        "  if (j% 10000)==0:\n",
        "    print(\"Error: \"+str(np.mean(np.abs(l2_error))))\n",
        "    \n",
        "  # Purpose: to obtain a 4x1 vector, l2_delta, containing the pos or neg values \n",
        "  #by which each element in syn1 (a 4x1) should be adjusted.  Each of these 4 \n",
        "  #\"tweaks\" improves the ability of syn1 to bring l2 closer ***to a local (or global?) minima.***Adam, is this correct?  Local, or global, or NEITHER?\n",
        "  \n",
        "  #First we take the derivative of all 4 elements of the 4x1 l2.  To take the deriv here\n",
        "  #simply means to determine the slope of the tangent to the point where each value lies\n",
        "  #on the S curve of the Sigmoid function.\n",
        "  \n",
        "  #YES to this Question: How does one number give us both rise and run to determine WHERE the \n",
        "  #number falls on the S curve? See Trask: does each l2 value represent the y axis,\n",
        "  #such that the 3 Trask dots on the S curve should be: (0.27, 0.5, and 0.9)?\n",
        "  \n",
        "  #Each derivative represents a confidence value, i.e., the values closer to 0 or to 1 mean \n",
        "  #the NN is very confident of its prediction.  \n",
        "  #\n",
        "  #AK: The above sentene is unclear.  Both the value of the sigmoid and its derivative are related to confidence, but the relationships are\n",
        "  #different.  For\n",
        "  #\n",
        "  #All values somewhere in the middle, around  \n",
        "  #.5 mean the NN is very wishy-washy, and not confident about its prediction.  As you see in the diagram of the S-curve, middling\n",
        "  #numbers have the steepest slope, i.e. a bigger value, which means nonlin(l1,True) will yield a relatively large value for that element in the vector l2_delta.\n",
        "  # For our purposes, we don't care about the highly confident numbers near 0 or 1.  Their slope is so shallow that their derivative will become a tiny weight in 12_delta, which will \n",
        "  #barely tweak syn1 when we update it below.\n",
        "  \n",
        "  #Question: for, say, 0.5, what numbers determine rise/run? What IS the slope of 0.5?\n",
        "  #Answer: Don't worry about this.  Let the computer use np.deriv to determine the slope and the derivative for you.\n",
        "  #The 4x1 vector of nonlin(l1,True) is multiplied element-wise by the 4x1 \n",
        "  #l2_error value.  The \"least confident\" values of nonlin(l1,True) are the\n",
        "  #biggest values, so they yield a relatively big value among the l2_delta elements\n",
        "  #that will eventually be multiplied by the syn1 elements when we update the synapses.\n",
        "  \n",
        "  l2_delta = l2_error*nonlin(l2,deriv=True)\n",
        "  # l2_delta is the error of the network scaled INVERSELY by the confidence. \n",
        "  # It's almost identical to the l2_error except that very confident errors are muted.\n",
        "  \n",
        "  # Purpose of the line below is back propogation: to determine: By how much did each l1 weight \n",
        "  #contribute to the l2 error (according to the weights)?\n",
        "  l1_error = l2_error.dot(syn1.T)\n",
        "  #Question: dot prod is rowXcol, so why is syn1 transposed?\n",
        "  #Question: I would have thought that l2_error DIVIDED by syn1 = l1_error...\n",
        "  #So: by multiplying l2_error by the weights in syn1, we can calculate the error in the middle/hidden layer?\n",
        "  \n",
        "  #Purpose of line below: uses the \"confidence weighted error\" from l2 to establish an \n",
        "   # error for l1. To do this, it simply sends the error across the weights \n",
        "   #from l2 to l1. This gives what you could call a \"contribution weighted error\" \n",
        "   #because we learn how much each node value in l1 \"contributed\" to the error in l2. \n",
        "   #This step is called \"backpropagation\" and is the namesake of the algorithm. \n",
        "   #We then update syn0 using the same steps we did in the 2 layer implementation.\n",
        "    \n",
        "     # Weighting l2_delta by the weights in syn1, we can calculate the error in the middle/hidden layer.\n",
        "  \n",
        "  #Line 43: uses the \"confidence weighted error\" from l2 to establish an \n",
        "   # error for l1. To do this, it simply sends the error across the weights \n",
        "   #from l2 to l1. This gives what you could call a \"contribution weighted error\" \n",
        "   #because we learn how much each node value in l1 \"contributed\" to the error in l2. \n",
        "   #This step is called \"backpropagating\" and is the namesake of the algorithm. \n",
        "   #We then update syn0 using the same steps we did in the 2 layer implementation.\n",
        "    \n",
        "\n",
        "  # In what direction is the target l1?\n",
        "  # Were we really sure?  If so, don't change it too much\n",
        "  l1_delta = l1_error * nonlin(l1,deriv=True)\n",
        "  # This is the l1 error of the network scaled by the confidence. Again, it's \n",
        "  # almost identical to the l1_error except that confident errors are muted.\n",
        "  \n",
        "  syn1 += l1.T.dot(l2_delta)\n",
        "  syn0 += l0.T.dot(l1_delta)\n",
        "print(\"Error after training:\")\n",
        "print(l2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UVeIH0B7ufg8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sigmund Freud ditty and memorizing code"
      ]
    },
    {
      "metadata": {
        "id": "T4AbF_lVXDMy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1) that converted number between 0 and 1 becomes a statistical probability and;\n",
        "2) it also becomes a confidence measure, and that's where Part 2 comes in.  Another part of this sigmoid function, i.e.,\n",
        "'  if(deriv==True):\n",
        "    return x*(1-x)'\n",
        "takes the confidence measure from Part 1) and converts it into **big advantage #3**, which is;\n",
        "3) a slope, and then;\n",
        "4) a weight with which to tweak the matrices of our NN and nudge them towards greater accuracy in prediction.  \n"
      ]
    },
    {
      "metadata": {
        "id": "ayNCMoXRGtoF",
        "colab_type": "code",
        "outputId": "3b39fc70-b267-4599-fac9-c82d39338598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def nonlin(x,deriv=False):\n",
        "  if(deriv==True):\n",
        "    return x*(1-x)\n",
        "  \n",
        "  return 1/(1+np.exp(-x))\n",
        "  X = np.array([[-0.1,4.5,2.7,1.1],\n",
        "              [0.1,7,6.9,7],\n",
        "              [9.4,6,6.2,5.8],\n",
        "              [9.6,8.5,10.4,8.1]])\n",
        "  l1= np.random.random((4,4))\n",
        "  \n",
        "print(l1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b11673a2de52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0ml1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'l1' is not defined"
          ]
        }
      ]
    }
  ]
}