{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dave teaches how to create NN on Github 02142019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidAcode/davidAcode.github.io/blob/master/Dave_teaches_how_to_create_NN_on_Github_02142019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hD5zvcQ7gBLc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Building a Neural Network: A Simple, Complete Explanation That Skips No Steps\n",
        "The best teacher is the person who just learned yesterday the stuff you are studying today, because she remembers what she struggled with and how she overcame it, and she can pass those shortcuts on to you.\n",
        "\n",
        "The best way to master Deep Learning is to master the fundamentals, build a neural network (NN), and then memorize the code, because then you can apply these fundamentals to any NN you meet in future academic papers or work projects.  It all starts here.  Today we will build a NN that can learn by trial-and-error: make a prediction, learn from its mistakes, and do better next time.  Our NN will solve a simple binary classifaction problem, i.e., \"given these 1's and 0's in the matrix X as inputs, what is your prediction that the 1's and 0's in matrix y will be?\"\n",
        "\n",
        "Thanks to super-teachers Andrew Trask and Siraj Raval, I mastered today's material--a working neural network in about 15 lines of Python--and it is the foundation of what you need to progress towards an understanding of the cutting edge techniques being used in deep learning today.  My mentor Adam Koenig, a Stanford PhD in aerospace engineering, helped me learn this stuff, and now I'm going to stand on these teachers' shoulders and help you learn it too.\n",
        "\n",
        "Here is the Big Picture: a diagram of the exact 3 layer neural network we will build today (For now, just focus on the bottom labels: \"Input Layer, Synapses, etc.\"  We'll get to the labels at the top later):\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/6r5olZyhL-0MUsWCMFFcNINVc5huehv53_oI9Q6M2yzR7BbFN7cS49INuFpnjU6n1sOt-J2Ezrp1elo-WilP8zjhgWOyv4JOtVu82WVFInrSoPn9L953LdrhaBb8o3-LUc7qFicKnG-nnvp61X_dKOMu9SY_KOPdEyJVsA1zeIlAPseY0jmnobn5SHt92Lp0ZHRDpWzFOzPkDImq49txsTGxpxBI6ylfyyWZTq3vUbS70G7f5uxeK31WKC8V319EsvhQG7yn2PnAWpQV9LXuqhcehkIDvL7NubbB0003OBwDEQF3phuLT-khP057JGbhse7dyxgF17imqmVK84CIYEnnXJI7JDvHaxsWd-xHN33PUglR9N4zqr5pv9AH4JwnvNEk94T9ewUEMyEscSLs1QD3Vf70XgqsMaWpRs66TIkwdBU-tmXoWxwR7bE-wgFUu96Z0XIBNk7aUKoZ6bpNg-DOJD5kTmagfQiTVCuGqhEIBzytGHFl1ta5gjPPTD4eT0kWeifLb6MquxMNpdnEPv57JYABBj5-8-pfeq8XV6wPhDCZ-cPIPH0YyDDMt0l21EnwTLsM8G_GYS07Le7VhPLQbqZWNmHxVsAZmGq1UtH9T0BavXJ2WwsptQsJy-2gb6621pOMR0GfAIMClbSF0QbjJzaHCmTK=w968-h579-no\n",
        ")\n",
        "\n",
        "Let me help you get your bearings in this diagram.  This is a 3-layer, feed-forward neural network.  The input layer is on the left: 3 neurons (sometimes known as nodes or features).  This layer is connected to the hidden layer (layer 1) by 12 synapses.  Layer 1 is then connected to the output layer, layer 2, by 4 synapses.  \n",
        "\n",
        "What does this diagram mean?  The network is saying to you, \"Based on the information you input into my left side, I will successfully predict the correct answer on my right side, the output.  I will do this in my hidden middle layer through trial-and-error: I will guess at the correct answer, compare my guess to the actual correct answer, then learn from my mistakes and improve, guessing better next time.  I will do this over-and-over 60,000 times, until I can predict near-perfectly what the outcome is of the information you input into me.\"\n",
        "\n",
        "Now, let's get an overview of our code.  I suggest you open this blog post in two side-by-side windows and show the code in the left window while you scroll through my explanation of it in your right window.  First, I'll show you the entire code we'll be studying today, and underneath that is my detailed step-by-step explanation of what it does.  Get ready for the wonder of watching a computer learn from its mistakes and recognize patterns!  We're about to give birth to our own little baby brain... :-)\n",
        "\n",
        "I am grateful for Andrew Trask's [blog post](http://iamtrask.github.io/2015/07/12/basic-python-network/) from which the code below is taken. Display this in your left window:\n"
      ]
    },
    {
      "metadata": {
        "id": "UohRpAqC_AuG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This is the \"3 Layer Network\" near the bottom of: \n",
        "#http://iamtrask.github.io/2015/07/12/basic-python-network/\n",
        "\n",
        "#First housekeeping: import numpy, a powerful library of math tools.\n",
        "import numpy as np\n",
        "#1 Sigmoid Function: changes numbers to probabilities and finds slope to use in gradient descent\n",
        "def nonlin(x,deriv=False):\n",
        "  if(deriv==True):\n",
        "    return x*(1-x)\n",
        "  \n",
        "  return 1/(1+np.exp(-x))\n",
        "#2 X Matrix: This is a set of inputs in the training set that we will use to \n",
        "#train our network.\n",
        "X = np.array([[0,0,1],\n",
        "              [0,1,1],\n",
        "              [1,0,1],\n",
        "              [1,1,1]])\n",
        "#3 y Vector: The output layer: Our training set of 4 target values. Once our NN\n",
        "#can correctly predict these 4 target values from the inputs provided by X, \n",
        "#it is now ready to predict in real life.\n",
        "y = np.array([[0],\n",
        "             [1],\n",
        "             [1],\n",
        "             [0]])\n",
        "#4 SEED: This is housekeeping. One has to seed the random numbers we will generate\n",
        "#in the training process, to make debugging easier.\n",
        "np.random.seed(1)\n",
        "\n",
        "#5 SYNAPSES: aka \"Weights.\" These 2 matrices are the \"brain.\" It learns, remembers, improves.\n",
        "syn0 = 2*np.random.random((3,4)) - 1 # 1st layer of weights, Synapse 0, connects l0 to l1.\n",
        "syn1 = 2*np.random.random((4,1)) - 1 # 2nd layer of weights, Synapse 1 connects l1 to l2.\n",
        "\n",
        "#6 FOR LOOP: this iterator takes our NN through 60,000 guesses, tweaks, and improvements.\n",
        "for j in range(60000):\n",
        "  \n",
        "  #7 FEED FORWARD NETWORK: Think of l0, l1 and l2 as 3 matrices as the \"neurons\" \n",
        "  #that combine with the \"synapses\" matrices in #5 to predict, improve, remember.\n",
        "  l0=X\n",
        "  l1=nonlin(np.dot(l0,syn0))\n",
        "  l2=nonlin(np.dot(l1,syn1))\n",
        "  \n",
        "  #8 TARGET, and how much we missed it by. y is a 4x1 vector containing our 4 target \n",
        "  #values. When we subtract the l2 vector (our first 4 guesses) from y, our target,\n",
        "  #we get l2_error: how much our neural network missed the target by on this iteration.\n",
        "  l2_error = y - l2\n",
        "  \n",
        "  #9 PRINT ERROR: in 60,000 iterations, j divided by 10,000 leaves a remainder of 0\n",
        "  #only 6 times. We're going to check our data every 10,000 iterations to see if\n",
        "  #the l2_error is reducing, and we're missing our target by less each time.\n",
        "  if (j% 10000)==0:\n",
        "    print(\"Avg l2_error after 10,000 more iterations: \"+str(np.mean(np.abs(l2_error))))\n",
        "\n",
        "  #10 In what DIRECTION is y, our desired target value, from our NN's latest guess? We\n",
        "  #take the slope of our latest guess, multiply it by how much that latest guess\n",
        "  #missed our target of y, and the resulting l2_delta tells us by what value to update\n",
        "  #each weight in our syn1 synapses so that our next prediction will be even better.\n",
        "  l2_delta = l2_error*nonlin(l2,deriv=True)\n",
        "  \n",
        "  #11 BACK PROPAGATION: After we \"fed forward\" our input in Step 7, now we work backwards\n",
        "  #to find the l1 error. l1 error is the difference between the ideal l1 that would \n",
        "  #provide the ideal l2 we want and the most recent computed l1.  To find l1_error, \n",
        "  #we have to multiply how much our latest prediction missed the target (l2_error) \n",
        "  #by our last guess at the optimal weights (syn1). We'll then use l1_error to update syn0.\n",
        "  l1_error = l2_delta.dot(syn1.T)\n",
        "\n",
        "  #12 In what DIRECTION is l1, the desired target value of our hard-working middle layer 1,\n",
        "  #from l1's latest guess?  Pos. or neg.? Similar to #10 above, we want to tweak this \n",
        "  #middle layer so it sends a better prediction to l2, so l2 will better predict target y.\n",
        "  #In other words, add weights to produce large changes in low confidence values and \n",
        "  #small changes in high confidence values\n",
        "  l1_delta = l1_error * nonlin(l1,deriv=True)\n",
        "  \n",
        "  #13 UPDATE SYNAPSES: aka Gradient Descent. This step is where the synapses, the true\n",
        "  #\"brain\" of our network, learn from their mistakes, remember, and improve--learning!\n",
        "  syn1 += l1.T.dot(l2_delta)\n",
        "  syn0 += l0.T.dot(l1_delta)\n",
        "\n",
        "#Print results!\n",
        "print(\"Our y-l2 error value after all 60,000 iterations of training: \")\n",
        "print(l2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6zpHJcEWoCQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you see from the comments above (the lines beginning with a #), I have broken this process of building a NN down into 13 steps.  Let's go through each step of the code in detail:\n",
        "\n",
        "#1) The Sigmoid Function: lines 6-11:\n",
        "\n",
        "\"nonlin()\" is a type of standard logistic function known as a Sigmoid function.  Logistic functions are very commonly used in science, statistics, and probability.  This Sigmoid function is written in a more complicated way than necessary here because it serves two functions:\n",
        "\n",
        "1) to take each of the matrices within its parentheses and convert each value to a number between 0 and 1 (aka a statistical probability).  This is done by line 11: `return 1/(1+np.exp(-x))` \n",
        "We will see below that this is very important, because this conversion to a 0-1 number gives us **FOUR** very **big advantages**.  I will discuss these four in detail below, but for now, just know that the sigmoid function converts every number in every matrix within its parentheses into a number between 0 and 1 that falls somewhere on the S-curve illustrated here:\n",
        "\n",
        "![alt text](https://iamtrask.github.io/img/sigmoid.png)\n",
        "\n",
        "(taken with gratitude from: \n",
        "[Andrew Trask](https://iamtrask.github.io/2015/07/12/basic-python-network/))\n",
        "\n",
        "So, Part 1 of the Sigmoid function has converted each value in the matrix into a number between 0 and 1, i.e., a statistical probability, which is also known as a confidence measure.  In other words, the number answers the question, \"how confident are we that this number correctly predicts an outcome?\"  \n",
        "\n",
        "Now for Part 2.  The second part of this sigmoid function is in lines 8 and 9:\n",
        "'  if(deriv==True):\n",
        "    return x*(1-x)'\n",
        "When called to do so by `deriv=True` in the code below, line 9 takes the confidence measure from Part 1 and converts it into a slope at a particular point on the Sigmoid S curve, which will be used to tweak the synapse matrices of our NN and nudge them towards greater accuracy in prediction.  \n",
        "\n",
        "So the sigmoid function plays a super-important role in making our NN learn, but don't worry if you don't understand it all yet.  I'll explain it in detail below.  Let's move on to Step 2:\n"
      ]
    },
    {
      "metadata": {
        "id": "WwxKZKxqwP0y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2) Creating X input: Lines 12-17\n",
        "Lines 12-17, step 2, create a 4x3 matrix of input values that we will use to train our network.  X will become layer 0, or l0 of our network, so this is the beginning of the \"toy brain\" we are creating!  \n",
        "\n",
        "Think of each row of this matrix as a training example we'll feed into our network, and each column is one node of our input.  So our Matrix X can be visualized as the 4x3 matrix in the top left-hand corner of the diagram below (don't worry about the other matrices yet):\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/5-n15xUnrfl0qjjyyuqEvrfXW5IgVyBfooAz6eiTofvKw_xZw1SB6Jk6g7g9sKrwsrIK65b1kWnqmJmG165m7FTGPv01uolyZ5F7ysCzdsRr4byLykg0r_VFPXBUJBzSfDtZKE1gA0wRTtC3iiaLqYdfMwa8cZMBv-xsxzGCLgBKv5AN-KAobPgKtz85AMh9DLbxyXrpKStQfmUM8v0Nksoduq8_KmKtlC_zABoDsMCYseZ7bDmNpUjgEy5jE8kEkIEhdgudhLLYRFW2U6PEubxebtX-g1uKGiMRXUX2vE0icw2q7NN48S4yrgK5Go2EjGW6NJqm-lQIsbjiinyeEOVatYDMhuYUjSzIyl9YZcWhoBT_viNZ5TVZrOt0ZzEDkfCpTvPCUK5jMdg8lZRpnT4y3htOFBDsVsET1DFx4e6OXN9PPo6250fb5QIx6IobAHhdGtWh5MEpoLblzrmIzrWC7Hy4xTvhUA5ZswF_EC4FpW6nGwxYBN0fhl4HcpnzsjgGqGrwFVqGGUocqZioHmgdsc-nxX5WOkYRAdPR9H5JnrMIPR6VwOCI6JUsQOIgEtnHodnjJYxzgAE1XuM9rqBUldRmnt-2iN2ySTq8_KTwdqLBsDpYjrWTkBf4AclZ8Z8cduPrt5yk9qkTNd7kPmIUvw_mmpYP=w968-h812-no)\n",
        "\n",
        "You may wonder, \"How does Matrix X become layer 0 in the diagram below?\"  We'll get to that soon.  Next, let's create our list of the four correct answers we want our NN to be able to predict."
      ]
    },
    {
      "metadata": {
        "id": "ze1bWVq0yJGD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#3) Create y output: Lines 18-24\n",
        "This code creates our output layer: Our training set of 4 target values. Once our NN can correctly predict these 4 target values from the inputs provided by matrix X above, it is now ready to predict in real life.  Think of X, the input layer, layer 0, as the beginning of our NN.\n",
        "\n",
        "#AK, Which is the \"output layer?\" l2? Or, the y vector?  Why?"
      ]
    },
    {
      "metadata": {
        "id": "2KbWE54s1ETU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#4) Seed your random numbers: Lines 25-27\n",
        "This step is housekeeping. We have to seed the random numbers we will generate in synapses/weights for the next step in our training process, to make debugging easier.  You don't have to understand how this codes works, you just have to include it."
      ]
    },
    {
      "metadata": {
        "id": "LOXeombt1uOL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#5) Create \"Synapses\" of your brain--Weights: Lines 29-31\n",
        "These 2 matrices are the \"brain\" of our NN.  These layers are the part of our NN that learn by trial-and-error making predictions, then improve their next prediction, then remember their improvements--learning!\n",
        "\n",
        "Notice how this code, `syn0 = 2*np.random.random((3,4)) - 1` creates a 3x4 matrix and seeds it with random values.  This will be the first layer of synapses, or weights, Synapse 0, that connects l0 to l1.  Congrats!  You just created matrix syn0 in our diagram below:\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/5-n15xUnrfl0qjjyyuqEvrfXW5IgVyBfooAz6eiTofvKw_xZw1SB6Jk6g7g9sKrwsrIK65b1kWnqmJmG165m7FTGPv01uolyZ5F7ysCzdsRr4byLykg0r_VFPXBUJBzSfDtZKE1gA0wRTtC3iiaLqYdfMwa8cZMBv-xsxzGCLgBKv5AN-KAobPgKtz85AMh9DLbxyXrpKStQfmUM8v0Nksoduq8_KmKtlC_zABoDsMCYseZ7bDmNpUjgEy5jE8kEkIEhdgudhLLYRFW2U6PEubxebtX-g1uKGiMRXUX2vE0icw2q7NN48S4yrgK5Go2EjGW6NJqm-lQIsbjiinyeEOVatYDMhuYUjSzIyl9YZcWhoBT_viNZ5TVZrOt0ZzEDkfCpTvPCUK5jMdg8lZRpnT4y3htOFBDsVsET1DFx4e6OXN9PPo6250fb5QIx6IobAHhdGtWh5MEpoLblzrmIzrWC7Hy4xTvhUA5ZswF_EC4FpW6nGwxYBN0fhl4HcpnzsjgGqGrwFVqGGUocqZioHmgdsc-nxX5WOkYRAdPR9H5JnrMIPR6VwOCI6JUsQOIgEtnHodnjJYxzgAE1XuM9rqBUldRmnt-2iN2ySTq8_KTwdqLBsDpYjrWTkBf4AclZ8Z8cduPrt5yk9qkTNd7kPmIUvw_mmpYP=w968-h812-no)\n",
        "\n",
        "The function np.random.random produces random numbers uniformly distributed between 0 and 1 (with a corresponding mean of 0.5).  But we want this initialization to have a mean zero.  Why?  So that the initial weight numbers in this matrix do not have an a-priori bias towards values of 1 or 0, because this would imply a confidence that we do not yet have (i.e. in the beginning, the network has no idea what is going on so it should display no confidence until we update it after each iteration).  \n",
        "\n",
        "So, how do we convert a set of numbers with an average of 0.5 to a set with a mean of 0?  We first double all the random numbers (resulting in a distribution between 0 and 2 with mean 1) and then we subtract one (resulting in a distribution between -1 and 1 with mean 0).  That's why you see `2*` at the beginning of our equation, and - 1 at the end: `2*np.random.random((3,4)) - 1`\n",
        "\n",
        "Notice that we are generating a 3x4 matrix.  Why?  Because l0 (aka our X matrix) is a 4x3, and matrix multiplication requires the inner 2 size numbers to match, i.e., a 4x3 matrix must be multiplied by a 3x_?_ matrix--in this case, a 3x4.  See how those inner two numbers must be the same?\n",
        "\n",
        "Then this line of code, `syn1 = 2*np.random.random((4,1)) - 1` creates a 4x1 vector and seeds it with random values (depicted with 4 question marks in the diagram).  This will be our NN's second layer of weights, Synapse 1, connecting l1 to l2.  Keep an eye on the size of each matrix we are creating (i.e., 4x3, 3x4, 4x4, etc.), because this will become *very* important soon.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3HiGfaOK9-ST",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#6) For Loop: Lines 33-34\n",
        "This is a for loop that will takes our NN through 60,000 iterations.  For each iteration, our network will take X, our input data, and based on that data, give its best guess at a prediction of what our y output is. It will then analyze how it did, learn from its mistakes, and give a slightly better prediction on the next iteration.  60,000 times, until it has learned by trial-and-error how to take the X input and predict accurately what the y output is.  Then our NN will be ready to take *any* input data you give it and correctly predict its future!\n"
      ]
    },
    {
      "metadata": {
        "id": "kCds2hNH_etM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#7) Feed Forward Network: Lines 36-40\n",
        "This is where our NN makes its first guess at a prediction. Think of l0, l1 and l2 as 3 matrices that are the \"neurons\" that combine with the \"synapses\" matrices we created in #5 to think, predict, improve, remember.  This is where matrix multiplication becomes key.\n",
        "\n",
        "First, on line 39 we multiply the 4x3 l0 and the 3x4 Syn0 to create (hidden layer) l1, a 4x4 matrix.  If you are new to matrix multiplication and linear algebra, fear not.  We're going to start simple, and break down our multiplication into tiny pieces, so you can get a feel for how this works.  Let's take one, single training example from our input.  Until now, we've been talking about l0 as a 4x3 matrix.  We're going to take the first example from that matrix, which would be, [0,0,1].  In other words, a 1x3 matrix.  We're going to multiply that by syn0, which would still be a 3x4 matrix, and our new l1 would be a 1x4 matrix.  Here's how that simplified process can be visualized:\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/wZspqVpfl87C6D9NY6YUzjoSvHyQol4iWaWAOr4aceo0u36X1kRdZ6xjuCnhUvtaCdxnkv6xsIEgxWgK5QYXGX4PSWKEtAuzxi5HnFAxM57RLSgC7sT3_mdPf5HOr2NcOqUXxgP1675YKE6KEcMFdSBV5NBaVSDmtvFu53xNNHlaB2o1vNEj8p6sIzErZ89ARP3VJJk0DVTenNw5-an7IThsPnecwTmqJshfcR11zT7fVNBNiiQEl0xUj7X-eDJioi9vF2A1r6qohfxTJlJDJ_1LRyARZwRRKCyWgXt_YS4TXPzmdt-TbOKCj9R2uRePN1sOgmy57U6l5N5RIVUsHDyidn2Fb7x6rbGwr508tfdwjAEC_HC5lK8tQKPC--fxrCZOJwjG_xNhI_EFevMybBbT0iq1cD6vG88IXejQAGSswW7yu0ogFanXcIiAV28gAomCv9ZAZy6Kki65XlhLptfQhBQk74lG-gtD9CCNQ4_ZNWpa3efw6VCOJou-g50cdKGBbQ2kux23IpuELF6Pn1Fbh4jqXJznPKxP2imMgOs7OlyRYFQHHKYhhaT9XFgGnqQSwvohr1Pq3cC6GRUSAJwxoo_qXALiAxryFHDaBihMpz0k4xi4xCrjcSEAPelthRCrqfKjbLla2jFAiGUusRWGV60rY7-P=w968-h709-no)\n",
        "\n",
        "So, the above diagram gives you a picture of how row 1 of our input X, which is one of four training examples, feeds through its first step in a NN.  But you may recall that we have 4 examples, not one.  The diagram below shows the same training example, [0 0 1] in its natural habitat--stacked on top of the other 3 training examples.  Once you can visualize how the neurons of example 1 (i.e., row 1 of Matrix X) are simply stacked on top of the other 3 rows/training examples of Matrix X, then you have the key to The Kingdom: hopefully, now you could visualize even a typical NN, with thousands or millions of neurons, containing thousands or millions of examples, stacked on top of each other.  The same image above still holds--there's just a whole lot of stacking going on!  These stacks are referred to as a \"full batch\" configuration, which is a very common model.\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/4cWnlTlN48LGR3Z5rHUWoWbE10sG5ON9KR7Kf-z6-tn8O0eFOKVunMQgcmj7NwlgN4VdGIQztiKEVNtd-ZX5asDDwkLVQCYC-uHeYKyZ0I1TYchAqhRq5TXl9Bur6-oesHMqvBPJKOOETh7wTEw6K7dt61eUKPoF2La-5Bzy7fZO3il9QxKjj3Uq7qznsVpwVQHXTQrvv-bY63qMmSFgEx20KfsOAyN1_PcmjkI5KRd0vsF6epUaL6kYfd1zQwrd65wS8ZVJxUggdvzO5pFdfxcv12SEtRQlxaYY1xSVOBM6MDF22gKMuUDE8gAZbhcGQI4ADlIwu1SX7n2PxBUkyN4Sm1eG5tR1uxQsoy1OGGy8JwBoARyVWuCi6UcgH9o-cH1ofPEWPiMyYb1QPJQHM33D6foEiFLzDc0a6nKQ8YBMUiqNs36KrusUX2ZfJv0Zkom6xahH1LTTQtub8H1jKcnrk_zhAZzJWwGuWNH6rXsBCFjo9YRyge9Z5F60SzGIlJwFVrqKRLqpVbCgIhYD9JMVZk0FzPZjNlpgsKYUDDpRs9H2P9hfPoQvuKrpAfqPNsfVSiL5jtcHOuGCt5XMrdZhMhg3ZS5N91N_OKa4FprHtrOwQYBBxcrqSKg-wE91vo28Wj-F4TNqfrBypRTMSKQAzu-g-ROs=w968-h810-no)\n",
        "\n",
        "The following diagram tells the same story from a matrix multiplication perspective.  Below is how the computer sees a neural network, and it's important for you to understand the visualization above and the computations below.  They both explain the same process of how neural networks function. (errata: my diagram should simply read, \"Feed forward: product...\" rather than \"dot product\").\n",
        "\n",
        "AK: Did I mess up my matrix multiplication? Below, it seems like my first row of \"l1 BEFORE nonlin()\" should have the values of -12.87, -5.18, 4.71, and -5.05.  Instead, I put those values in Column 1.  Please confirm.\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/Qbej-Y21C2su8KnB4dzus7l39oJ8c1VZ-3oLOJLqWrb6sr_pAjyq2vBPJ8Y8VG8JPaQQKOybzyTo7sqgDI6OPdd24VAR71aMXSzXRpaPpxLsYwgvEhWJxC0CwtUIlJp5Xjar-jvUMBrybLnnsbhJ9s-cf1Exci0nlUNFCiFjtH1O5SdST71fPXhZoUUOxN9-hWv6Msr5Obpg0ve5RrZc_VvSGD9I85nqxRvpaILZkQQLeggj4tlBW528lymNR9e28gwOy9fuAopRaM37Mfw0or9aIZiK4jjiSuU0FoHX757Mv0JgvmM6QgVOb2sNxpX6Lx2eASFX6nxctGwXW0hX1wnzZRRf_KttjjeOTSa6r70KZdKy3f8RH4JK3oxroGBI4RXWOaSwNHHgmd7ljoo0uvUmKy3ScE0cMummCWfTNAv8dcvvi53JqbixVglUL_DQ-v-P4ZyW5HY2xJTWZ8KgByme0FlZTQFURPkvHFw7g375kI3Ad188o5L9p30SlvyKZ_N5Dp2VXHS5cavs42-OVWY47v0fYijR8VpPBOAfWNrcVAn1jOO_YLOwv7z3VFc8RbyMSoAFve0b3m6qGNHh-oYvpVhAuelaUzRWb5x0HYSLUq-wQZRksy8klBo2LWhQxsYTFoZSKLocyNj83uBnCd5-lrAve-SU=w968-h594-no)\n",
        "\n",
        "Notice that line 39 uses the `np.dot()` syntax to multiply matrices.  Later we'll use the simpler multiplication syntax, i.e.,`(*)` when we are doing element-wise multiplication of two vectors.  But you must be clear on the size of matrices you are multiplying in order to use the right syntax.\n",
        "\n",
        "Still on line 39, we next take the Sigmoid function of l1 because l1 may have values between 1 and -1, and we need it to have values between 0 and 1, hence: `l1=nonlin(np.dot(l0,syn0))`\n",
        "\n",
        "It is on line 39 that we see ***Big Advantage #1*** of the ***Four Big Advantages of the Sigmoid Function.***  When we pass the dot product matrix of l0 and syn0 through the `nonlin()` function, the sigmoid converts each value in the matrix into a statistical probability between 0 and 1.  This means, \"the closer the value is to 1, the more certainty there is that such-and-such is the case, whereas the closer the value is to 0, the more certainty that such-and-such is NOT the case.  \"So, what?\" you may ask.  Well, it doesn't matter in lines 39 and 40, but it matters a *ton* when we hit line 61 and beyond.  Stay tuned.\n",
        "\n",
        "Exactly the same thing happens on line 40, as we take the dot product of 4x4 l1 and 4x1 syn1 and then run that product through the Sigmoid function to produce a 4x1 l2 with each value becoming a statistical probablility from 0-1.  Note that all numbers in the matrices below are fake--they don't follow from the diagrams above (errata: my diagram should simply read, \"product\" rather than \"dot product\").\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/3J2E3TRyLA2YCqF6XcHXn22e1qb_8U-2OTVUGZmgPjm1Y4AiAwOjID8CVP1tpRC2vKKWX2Sk3gQtA0Efxq7dURbFjirsdSFJJPJ3G1pH0LHh771mGRqr7DHNQ7521JxpdFiyhP5jCA8_K-mtRdctF316SRtg9M9Ni0TNuR4FonhSjZmI2oP1o9azJdmsoyEVHhJF6TcsdNllYoNGlsgioEwaIidkm2B6a8L-TEkRJjYc6nzJzTG0Z8uQcrxl5xpCrj-YO-F2r8a0gMZur0w1NJm6LLA1kbMDipdGk9IiDbxwLpNd-9jyLiFZRsOZsc2SB18-TCjWSO0oqXlLTr86XtDPp2Xh-fKseNWeFYmjyWIEY20DI7ggDr2jJgLrOR-kJPA9hMpk_xOHOyWgYfn3XF7QPnO_bKUsvLJ0FPUyICsAV1IVQQm0e-K7QdDmRRHioE7Ri4NOt-3THjBi3D4P3pCtzha9aSqxNG3F2AXKb4ziD3Dm0GBei-EY3Vjoa4J--0XRwSH96L2unpiiv5cpPbyGXvraWvHb1ODdPiD73xDLH1R457gFTccsLq0Fuq0b510mDaFi_zIdCME8_ifdgNxj9oSBSo7U690-BKb6svQEz3LEyQnDrcBhEHZW6bSN3H1JAQvUmurC8ylt1BzBgZimM3EFzH7h=w968-h632-no)\n",
        "\n",
        "We have now completed the Feed Forward portion of our network.  If you can visualize what we have done so far, both in terms of the matrices involved, and also as layers of \"neurons\" and the \"synapses\" connecting those neurons, then you have done outstanding work.  Bravo to you.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jHKp8pPNiiX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#8) By How Much Did We Miss the Target? Lines 42-45\n",
        "The 4x1 y vector is our goal, our target.  Given our input X of layer 0, we want to produce an output, layer 2, that is as close to the 4 values of y as possible.  Each one of our 60,000 iterations should bring us, by trial-and-error and learning from our mistakes, closer to the 4 target values of y.  So, for each iteration, we take our best prediction so far, the 4x1 vector l2, and subtract it from the 4x1 vector y.  The remainder is l2_error, i.e., how much each value of l2 missed its target value in y.  \n",
        "\n",
        "This is the exciting first step in the learning process of our NN.  Once we know what we missed by, in the following steps we will seek to correct that error and do better next time."
      ]
    },
    {
      "metadata": {
        "id": "oU2-ZoUJlQWd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#9) Print Error: Lines 47-51\n",
        "Line 50 is a clever parlor trick to have the computer print out our l2_error every 10,000 iterations.  The line, `if (j% 10000)==0:` means, \"If your iterator is at a number of iterations that, when divided by 10,000, there is no remainder, then...\"  ` j%10000 `would have a remainder of 0 only six times: at 0 iterations, 10,000, 20,000, and so on to 60,000.  So this print-out gives us a nice report on the progress of our NN's learning.\n",
        "\n",
        "The code `+ str(np.mean(np.abs(l2_error))))` simplifes our print out by taking the absolute value of each of the 4 values, then averaging all 4 into one mean number and printing that."
      ]
    },
    {
      "metadata": {
        "id": "C810qURdm3hZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#10) In What DIRECTION is y?  Lines 53-57\n",
        "Now we have entered the brain of the beast; here is the secret sauce of Deep Learning.\n",
        "\n",
        "You might call Step 10, \"How much do I tweak my NN before its next iteration and prediction?\"  For statistics and calculus buffs, we could simply say, \"In line 61 we compute how much the l2_delta needs to modify the error with weights from the derivatives to induce large changes in low confidence values and smalll changes in high confidence values.\"  Whew!  For the rest of us mere mortals, let me unpack that a bit:\n",
        "\n",
        "First, let me give you an overview of what we're going to do:\n",
        "\n",
        "1) In line 45, we find l2_error, which measures how much our first prediction, l2, missed the target values of y;\n",
        "\n",
        "2) In line 61, we use l2_error to calculate by how much we need to adjust the weight values in our syn1 matrix (and in which *direction*--positive or negative) in order to make a better prediction next time.  l2_delta = \"how much to change the weights, + or -, in line 81.\"  \n",
        "\n",
        "3) Rinse and repeat: in line 68, we will find the l1_error and use it in line 77 to calculate the l1_delta by which we should change syn0 in line 82;\n",
        "\n",
        "Now, the details:\n",
        "\n",
        "Here is where you will see the beauty of the Sigmoid function in four *magical* steps.  To me, the genius of our neural network's ability to learn is found largely in these four steps.  We saw how, in line 39, Step 1 was when the `nonlin()` transformed each value of our matrix into a statistical probability (i.e., a prediction) between 0 and 1.  But I have yet to mention that that statistical probability is ***also*** a simple measure of confidence--numbers approaching 1 suggest high confidence that the NN's (neural network's) prediction of \"1\" is correct.  This is ***Big Advantage #2*** of the ***Four Big Advantages of the Sigmoid Function:***  Numbers approaching 0 suggest high confidence that the NN's prediction of \"0\" is correct.  If our NN's prediction, the four values of l2, is high-confidence and high-accuracy, that's an oustanding prediction, and we want to leave the syn0 and syn1 weights that produced that oustanding prediction alone.  We don't want to mess with what's working; we want to fix what's NOT working.\n",
        "\n",
        "Let me explain the above from a different angle: right now, our NN is dealing with a pretty abstract problem, i.e., 1's and 0's.  Let's give those 1's and 0's meaning: imagine that our problem is one of image recognition, e.g., \"Is there a fish in this image?  \"1\" means, Yes, and \"0\" means No. Within this context, the output is simultaneously a prediction and a confidence measure.  An output of 0.999 is the equivalent of the network saying \"I am extremely confident that there is a fish in this picture\".  A number of 0.001 is the equivalent of \"There is definitely not a fish in this picture\".  Low confidence numbers are in the vicinity of 0.5.  For example, a value of 0.4 would be similar to \"I don't think there is a fish in this picture, but I'm not sure.\"\n",
        "\n",
        "That's why we focus our attention on the numbers in the middle:  all numbers approaching 0.5 in the middle are wishy-washy, and lacking confidence.  So, how can we tweak our NN to produce four l2 values that are both high-confidence and high-accuracy?  \n",
        "\n",
        "The key lies in the values, or ***weights*** of syn0 and syn1.  As I mentioned above, syn0 and syn1 are the center, the absolute *brains* or our neural network.  We are going to take the four values of the l2_error and perform beautiful, elegant math on them to produce an l2_delta.  l2_delta means, basically, \"the amount by which we need to increase-or-decrease our weight values in syn0 and syn1 in order to reduce wishy-washyness and maximize the high-confidence, high-accuracy values of the prediction of our next iteration.\"\n",
        "\n",
        "***Get ready for beauty.***\n",
        "\n",
        "Here is ***Big Advantage #3*** of the ***Four Big Advantages of the Sigmoid Function:*** Do you remember that diagram of the beautiful S-curve of the Sigmoid function that I showed you above?  Well, lo-and-behold, each of the 4 probability/confidence values of l2 lies somewhere on the S curve of the sigmoid graph (pictured again below, but this time with more detail).  If we search for that number (e.g. 0.9) on the Y axis of the graph below, we can see that it corresponds with a point on the S curve roughly where you see the green dot: ![alt text](https://iamtrask.github.io/img/sigmoid-deriv-2.png)\n",
        "(taken with gratitude from: \n",
        "[Andrew Trask](https://iamtrask.github.io/2015/07/12/basic-python-network/))\n",
        "\n",
        "Did you notice not only the green dot but also the green line through the dot?  That green line is meant to represent the slope of the *tangent* to the line at the exact point where that dot is.  You don't need to know calculus to take the slope of a curve at a particular point--the computer will do that for you.  But you do have to notice that the S curve above has very shallow slope at both the upper extreme (near 1) and the lower extreme (near 0).  Does that sound familiar?  Wonder of wonders, a shallow slope on the sigmoid curve coincides with high confidence and high accuracy in our predictions!  And you also need to know that a shallow slope on the S-curve comes out to a tiny number for slope.  That's good news.  Why?\n",
        "\n",
        "Because, when we go to update our synapses, we basically want to leave our high confidence weights alone since they already have good accuracy.  To \"leave them alone\" means to multiply them by tiny numbers, near zero, so the values remain virtually unchanged.  And here comes ***Big Advantage #4*** of the ***Four Big Advantages of the Sigmoid Function:*** Miracle-of-miracles, our high-confidence numbers correspond to shallow slope on the S-curve, which corresponds to tiny slope numbers, so therefore multiplying the values of syn0 and syn1 by these teeny-tiny numbers has exactly the effect we want: the values in our synapses are left virtually unchanged, so our confident, accurate, high-performing values in l2 remain so.\n",
        "\n",
        "By the same token, our wishy-washy, indecisive, low-accuracy l2 values, which correspond to points in the middle of the S-curve, are the numbers that have the biggest slope on our S-curve.  What I mean is, the values around 0.5 can be traced on the Y axis of our graph below to the middle of the S-curve, where the slope is steepest, and therefore the value of that slope is a big number.  Those big numbers mean a big change when we multiply them by the wishy-washy values in l2, as we do in line 61.\n",
        "\n",
        "##Super Key Point: Positive-or-Negative Direction, and Gradient Descent\n",
        "When we update our synapse matrix by multiplying its corresponding element (aka, its value or number) with that large slope number, it's going to give that element a big nudge in the right direction towards confident and accurate prediction.  When I say, \"in the right direction,\" what I mean is that some values of our l2_delta are going to be negative, because we want them to reduce the weight values, so that when we multiply those reduced weights by the corresponding element in syn0 and syn1, it nudges those elements to approach 0.  Other values of our l2_delta are going to be positive, because we want them to increase the weight values and thereby nudge the elements in syn0 and syn1 to approach 1.\n",
        "\n",
        "So it's important to notice that there is a sense of \"direction\" involved here.  When we talk about \"what direction is the y value from our current l2 value?\" we mean, do we need to multiply each weight in syn1 by a positive l2_delta value to move it in a positive, larger direction, or by a negative l2_delta value to move it in a negative direction?\n",
        "\n",
        "Have you heard of gradient descent?  Gradient descent is all about direction.  It is often described as, \"a ball dropped in a bowl and rolling in a **back-and-forth direction** until it comes to a rest at the global minimum, the bottom of the bowl.\"  That's what the Sigmoid does for us.  It helps us to find the bottom of the bowl, which minimizes the cost function, which minimizes the error in our predictions.  Think of our 60,000 iterations as the ball rolling in a back-and-forth direction in the bowl until it no longer needs to change direction because it has come to rest at the ideal, perfect bottom of that bowl, where error as at a minimum, and life is good.  Picture it like this:\n",
        "\n",
        "![alt text](https://mail.google.com/mail/u/0?ui=2&ik=e3f869f938&attid=0.2&permmsgid=msg-a:r4352876950048414936&th=1691255aa52a4d54&view=fimg&sz=s0-l75-ft&attbid=ANGjdJ8FdFORGv3w0jn-Bs8GhlKpg2D1XPRzSF6OaNCqE8hchNYMIAymIg-nK1xCdIsQup54rJmkW2l0qttCzg03Hq8PJOv4KX0ae14e2dkswvLMt74Rzdhwt2ZJQBQ&disp=emb&realattid=ii_jsexnu8o2)\n",
        "(taken with gratitude from [Grant Sanderson](https://www.youtube.com/watch?v=IHZwWFHWa-w&t=2s&index=3&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) Ch. 2)\n",
        "\n",
        "Above is a nice, simple picture of the \"rolling ball\" of gradient descent.  Line 61 computes each value of l2 as a slope value.  Of the 3 \"bowls\" pictured in the diagram above, it is clear that the true global minimum is the deepest bowl on the far left.  But, for simplicity's sake, let's pretend that the bowl in the middle is the global minimum.  So, a steep slope downwards to the right (i.e., a negative slope value, as depicted by the green line in the picture) means our ball will roll a lot in the negative direction (i.e., to the right), causing a big, negative adjustment to the corresponding weights of syn1 that will be used in the next iteration to predict l2.  But if, for example, you have a shallow slope downwards to the left, that would mean the prediction value is already accurate and confident, which produces a tiny, positive slope value, so the ball will roll very little to the left (i.e., in a positive direction), thus adjusting by very little the corresponding weight in syn1, so the next iteration's prediction of that value will remain largely unchanged.  This makes sense because the back-and-forth motion of the rolling ball is becoming smaller and smaller before it soon comes to rest at the global minimum, the bottom of the bowl, so there's no need to move much.  It is already close to the ideal.\n",
        "\n",
        "The above 2-dimensional diagram is a tad oversimplified, so here's a more accurate picture of what gradient descent looks like:\n",
        "![alt text](https://lh3.googleusercontent.com/jIup60T65tIKtXg0B-Np6jeNXk4TvQTRgBI1btNRZUZ4yy_ZEyL1bN3RwiSjzKNcbyXQN6z7vdV55NzGFxJfUpZXkyU6HTmrScht0rbk5BXGC6eO79LrZuuVpJdHE4fr4QYwvdbO)\n",
        "(taken with gratitude from [Grant Sanderson](https://www.youtube.com/watch?v=IHZwWFHWa-w&t=2s&index=3&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) Ch. 2)\n",
        "\n",
        "Think of the dotted white line in the diagram as the path our gradient descent ball takes toward \"The Bottom of the Bowl where Accurate Prediction Lives.\"  Each dot of that dotted white line represents a tweak, or update of the weights syn0 and syn1, that will take our ball closer-and-closer to the bottom of the bowl, which is the global minimum error, where our NN's predictions are most accurate.\n",
        "\n",
        "Take your time with the above points and make sure you understand them.  Do you see why the sigmoid function is a thing of beauty?  It takes any random number in one of our matrices and:\n",
        "\n",
        "1) turns it into a statistical probability, \n",
        "\n",
        "2) which is also a confidence level, \n",
        "\n",
        "3) which turns into a big-or-small tweak of our synapses, and \n",
        "\n",
        "4) that tweak is always in the direction of greater confidence and accuracy.  \n",
        "\n",
        "The sigmoid function is the miracle by which mere numbers in a matrix can \"learn.\" A single number, along with its many colleagues in a matrix, can represent probability and confidence, which allows a matrix to \"learn\" by trial-and-error.  That is a thing of beauty.\n"
      ]
    },
    {
      "metadata": {
        "id": "Du-KhNydusM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#11) Back Propogation: Lines 59-64\n",
        "In Step 8 we found out by how much our prediction, l2, missed our target values of y, and we called it the l2_error.\n",
        "In Step 10 we used l2 and l2_error to compute the l2_delta.  l2_delta means, \"by how much, positively or negatively, should we tweak the weights of syn1?\"\n",
        "Now, in Steps 11 and 12, we are going to compute the l1_delta, but in a slightly different manner.  We are going to use back propogation.  For now, think of it as \"working backwards.\"  Here we go: \n",
        "\n",
        "Consider this: back when we were finding l2_error, life was easy.  We knew l2, and we knew \"The Ideal l2\" we are shooting for, which was simply y.  But with l1_error, things are different.  We don't know what \"The Ideal l1\" is.  There's no y to tell us that.  y only tells us The Ideal l2.  So, how are we going to figure out what The Ideal l1 is?  We're going to have to take what we *do* know and work backwards.\n",
        "\n",
        "We know l1.  We know syn1.  We know l2_error.  We know l2_delta.  How do we use those to discover what The Ideal l1 is, so we can subtract our current l1 from it to compute our l1_error?  \n",
        "\n",
        "The answer is, we don't *have* to know what The Ideal l1 is.  We only care about finding the l1_error.  Of course, when finding l2_error we simply subracted l2 from y.  But in the case of l1, we can jump straight to finding the l1_error in one step, without any subtraction.  How?\n",
        "\n",
        "AK: below are my best 2 guesses as to how Back Propogation works.  Please edit or combine them to give me one, shiny explanation of back prop:\n",
        "GUESS 1: Well, think about what l2_delta really is.  It is the l2_error times the slope of l2.  And the slope of l2 is just the confidence levels of l2, right? So l2_delta is like error-times-confidence.  In other words, l2 error x l2 confidence.  That's what l2_delta is.  And we got the confidence levels of l2 by multiplying l1 by syn1.  So, you might say syn1 \"made a contribution\" to l2 confidence.  If you could separate syn1's contribution from l2's confidence, you'd be left with the l1_error.\n",
        "\n",
        "So, what if you divide (l2 error x l2 confidence)?  syn1 contributed to both.  It was because of syn1 that the l2 error is what it is, and it was also because of syn1 that l2 confidence is what it is.\n",
        "\n",
        "So l2_delta tells us how much (or how little) to multply syn1 by next time to create a better l2, with smaller l2_error.\n",
        "\n",
        "GUESS 2: Well, consider this: we have our most-recently computed l1, but it's not perfect.  The ideal l1 would have provided the easiest path to creating an ideal l2 when we multiplied l1 x syn1.  But alas, the ideal world did not happen, because we're learning by trial-and-error here.  So, we want to know: by how much did our current l1 miss the ideal l1?  That difference is the l1_error.  \n",
        "\n",
        "And the only way to find that l1_error (which we don't know yet) is to work backwards from our current l2_delta and our current syn1 (which we do know).  \n",
        "\n",
        "In line 64, we first multiply our \"old syn1\" (the weights already used in the feed-forward part of this current iteration) times the l2_delta.  \n",
        "In that feed-forward part, when we multiplied l1 by syn1, the weights of syn1 were our best guess of what each l1 value needed in order to become a shiny, perfect l2 value.  But the l2 value missed the y value by amount *X*, and if we multiply that amount X by the big-confidence=small-slope derivative of l2, \n",
        "\n",
        "Think of back propogation as a kind of \"Division Parlor Trick\": we know the values of l1, and we know the values of syn1.  But we don't know what the values of the ideal l1 are, so we can't just subtract l1 from the mystical Ideal l1 to get our l1 error.\n",
        "\n",
        "This is indeed working backwards, because it was this version of syn1 that gave us our current l2 prediction, which produced our current l2_error.  So, to multiply the l2_error by the old syn1 allows us to compute by how much the old l1 of this iteration missed the ideal l1.  \n",
        "\n",
        "Why?  Consider: if the old l1 had been perfect, then multiplying this perfect l1 by the old syn1 would have produced a better l2, with less l2_error.  But the old l1 wasn't perfect, therefore the l2_error is bigger than it could have been, so multiplying it by the old syn1 gives us a clear picture of by how much the old, imperfect l1 missed its ideal l1--the l1_error.  In other words, we learn by how much each value in l1 contributed to each corresponding value in the l2_error.\n",
        "\n",
        "We'll then use l1_error to update syn0 below.\n",
        "END OF GUESSES 1 and 2\n",
        "\n",
        "AK, Trask describes this back proppgation like this: \n",
        "l1_error = l2_delta.dot(syn1.T)\n",
        "The above line of code uses the \"confidence weighted error\" from l2 to establish an error for l1. To do this, it simply sends the error across the weights from l2 to l1. This gives what you could call a \"contribution weighted error\" because we learn how much each node value in l1 \"contributed\" to the error in l2. This step is called \"backpropagating\" and is the namesake of the algorithm. We then update syn0 using the same steps we did in the 2 layer implementation.\n",
        "AK, I don't understand what Trask means by, \"sends the error across the weights from l2 to l1.\"  Can you explain?\n",
        "\n",
        "AK, this is the section where I have the least confidence.  What am I missing?  What should I add?  What diagram would be helpful?\n"
      ]
    },
    {
      "metadata": {
        "id": "z_G-5l23bTcI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#10 In what DIRECTION is y, our desired target value, from our NN's latest guess? We\n",
        "  #take the slope of our latest guess, multiply it by how much that latest guess\n",
        "  #missed our target of y, and the resulting l2_delta tells us by what value to update\n",
        "  #each weight in our syn1 synapses so that our next prediction will be even better.\n",
        "  l2_delta = l2_error*nonlin(l2,deriv=True)\n",
        "  \n",
        "  #11 BACK PROPAGATION: After we \"fed forward\" our input in Step 7, now we work backwards\n",
        "  #to find the l1 error. l1 error is the difference between the ideal l1 that would \n",
        "  #provide the ideal l2 we want and the most recent computed l1.  To find l1_error, \n",
        "  #we have to multiply how much our latest prediction missed the target (l2_error) \n",
        "  #by our last guess at the optimal weights (syn1). We'll then use l1_error to update syn0.\n",
        "  l1_error = l2_error.dot(syn1.T)\n",
        "\n",
        "  #12 In what DIRECTION is l1, the desired target value of our hard-working middle layer 1,\n",
        "  #from l1's latest guess?  Pos. or neg.? Similar to #10 above, we want to tweak this \n",
        "  #middle layer so it sends a better prediction to l2, so l2 will better predict target y.\n",
        "  #In other words, add weights to produce large changes in low confidence values and \n",
        "  #small changes in high confidence values\n",
        "  l1_delta = l1_error * nonlin(l1,deriv=True)\n",
        "  \n",
        "  #13 UPDATE SYNAPSES: aka Gradient Descent. This step is where the synapses, the true\n",
        "  #\"brain\" of our network, learn from their mistakes, remember, and improve--learning!\n",
        "  syn1 += l1.T.dot(l2_delta)\n",
        "  syn0 += l0.T.dot(l1_delta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wHY7OJ7g4NdU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#12) In what DIRECTION is the target (ideal) l1?  Lines 66-69\n",
        "We will use the exact same process as Step 10 to find in what direction our gradient descent should be moving in order to take us closer to the perfect l1 that will contribute to us finding the perfect l2, our ultimate goal.\n",
        "\n",
        "We want to answer the question, \"In what DIRECTION is l1, the desired target value of our hard-working middle layer 1, from l1's latest prediction in this current iteration?  We want to tweak this middle layer of our NN so it sends a better prediction to l2, making it easier for l2 to better predict target y.  In order to answer this question, we need to find the l1_delta, which tells us how much to adjust the weights to produce large changes in low confidence values and small changes in high confidence values.\n"
      ]
    },
    {
      "metadata": {
        "id": "_dyqSFWe6JDj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#13) Update Synapses, aka Gradient Descent: Lines 71-74\n",
        "This final step is all the Glory Moment:  all our work is complete, and we reverently carry our hard-earned l1_delta and l2_delta up the steps of the podium to our hallowed leader, Emperor Synapse, the true brains of our operation.  \n",
        "\n",
        "We update syn1 and syn0 so they will learn from their mistakes of this iteration, and in the next iteration they will lead us one step closer to that ideal bottom of our bowl, where error is smallest, predictions are most accurate, and joy abounds!\n",
        "\n",
        "AK, why do we multiply the l2_delta by ***l1*** to update syn1?  I would have expected to multiply the *old* syn1 by the l2_delta to update the weights by the corresponding delta in l2_delta, thus creating a new-and-improved syn1 to use in the next iteration.  No?  Why not?\n",
        "syn1 += l1.T.dot(l2_delta)\n",
        "syn0 += l0.T.dot(l1_delta)\n"
      ]
    },
    {
      "metadata": {
        "id": "UuWimXqfBYaz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#In Closing...\n",
        "Andrew Trask gave me a fabulous gift when he wrote that memorizing these lines of code leads to mastery, and I agree for two reasons: \n",
        "\n",
        "1) When you try to write out this code from memory, you will find that the places where you forget the code are the places where you don't understand the code.  Once you understand this code perfectly, every part of it will make sense to you and therefore you will remember it forever;\n",
        "\n",
        "2) This code is the foundation on which (perhaps) all Deep Learning networks are built.  If you master this code, every network you learn and every paper you wade through will be clearer and easier because of your work in memorizing this code.\n",
        "\n",
        "Memorizing this code was made easy for me by making up an absolutely ridiculous story that ties all the concepts together in a fairy-tale mnomonic.  You will remember better if you make your own, but here's mine, to give you an idea.  I count the 13 steps on my fingers as I recite this story out loud:\n",
        "\n",
        "1) Sigmund Freud (think: Sigmoid Function) abolutely *treasured* his neural network, and he buried it like a pirate's treasure, \n",
        "\n",
        "2) \"X\" marks the spot (Creating X input that will become l1).  \n",
        "\n",
        "3) \"Why,\" I asked him (Create the y vector of target values), \"didn't you plant \n",
        "\n",
        "4) Seeds instead?\" (Seed your random number generator)  \"You could have grown a lovely garden of \n",
        "\n",
        "5) Snapdragons,\" (Create Synapses: Weights) \"which could be fertilized by the \n",
        "\n",
        "6) firm poop\" (For loop) \"of the deer that \n",
        "\n",
        "7) Feed on the flowers\" (Feed Forward Network).  Suddenly, an archer \n",
        "\n",
        "8) Missed his target (By How Much Missed the Target?) and killed a grazing deer.  As punishment, he was forced to \n",
        "\n",
        "9) Print his error (Print Error) on a 500 times on a blackboard facing the \n",
        "\n",
        "10) Direction of his target (In What Direction is y?).  But he noticed behind the \n",
        "\n",
        "11) BACK of his target two deer were mating and PROPOGATING their species (Back Propogation) and he shouted for them to stop but they wouldn't take \n",
        "\n",
        "12) Direction and ignored him (In what Direction is the l1 target?).  He got so angry that his mind \n",
        "\n",
        "13) Snapped and he descended into Gradient Insanity (Update Synapses, Gradient Descent).  \n",
        "\n",
        "So, there is a very silly story, but I can tell you that it has burned those 13 steps into my brain, and once I can write down those 13 steps, and I understand that code in each step, to write the code perfectly from memory becomes easy.\n",
        "\n",
        "I hope you can tell that I love my journey into Deep Learning, and I wish you the same joy I find!\n",
        "Feel free to email me improvements to this article at: DavidCode1@gmail.com"
      ]
    },
    {
      "metadata": {
        "id": "-NH3m8DwACz0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "extra diagrams:\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/Rsh6bYtBEu_oAMLVXppmxO0ayEBDJVQg8YwDZGz0_I6JKB588v5aVQbzQKrx-srfY1sGZm4V83NuSIPxaWBntZH3Hm584xbax4RR0f3L5IAOLSB3jH5ZNXJnnhl7gAQR9w2UsqOuxsdoRDXj1xrmLZkC_ZCEZrooNcHJRpEPySTsTiTb61j9AQg-uH2TLUewt02YPshI4i0f3Ynoc9IHa6IgywBH_k_Baae6FcOIuTPApdSiLDviMoZuIZLMOdGusJ1oV_E4sl7wWHcr4JQOlpz8rGArnyLkUywKh_1BJigH_z8s1EcUuA6NfUY8W_4qGJsEqLKDac2hhvvgqoUQTiLqis7PKp_8eS-MjpSv5tz17gclQAj4snNmlF_gEKLWMgRuPVju1zGmoMZeMPqMnXDE-QWVVAIKFYwtCe83KELSDKmkRCI_hySi0MYualR9tL7EPRdSjpS6VWoChupgnHodSdPKmNkx7qcGI8vE-BMWiVzxC1gxICM9DUc5h7LCAkwR2p4tN94tQB8z12AxI5-TI8l_tT1-RvgrhqO5QJv2tBzud7P6rJBUZxSVHqCt4wlP4ZbVgAviCwJDpzLzbsI3HE9666hHMNZLcUqsTd2YRwaV-p7m8DTcDmy4IzB36qNMDSNip8VG4CunowqCbQlw6kXBzVYp=w968-h698-no)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/1cxj3K8bj_IOL8lhF9pRQSbDQyo3imZGVmZlEsjfvX1ImXBIJZHeJgOtX2kOQoaEbfhCbqtR2JFzSXK_r41dGGVdH6rde7QgoIqEGGn-v37CcD6IWw-nk9put1txPGeKvP-fxo3Lwk3nlwd3nkIdgjMu8HQf9ccyAZGlgRj7YH3avElDzwJr6kEDygOaLxmR9xo427aQHMwSjhdQKrrHwfTSsrUZmXyKh5F2anucPUAB-BAfTNyIG4brluYZwwKq3Mhx4eOJkn51V0D4zlcDVDPEK7k789ZUYVSfEcJ4_QbIY7YVeSHkqO-jQ3XB6XeK1WydnTFF8-OcOgBEzuKB6LDUNUYttH8eKE-k7OB5vi7h56PmIK35-ciOix52zaQRP0s8kcBeL5NAimjhIZjC6j5Am7CAf7VeBZUs9q9UvemlNYRZze_SWmlJIQBVx05GOShiGlHXAYZxdEWSOT9P4B09RmeqVCxakgQbgzOpS-05KVFzcIDn8HYsKOLj05uURaLfPL8AP3kKEOSE4z3iSdBxVnRuu6taQcRBU9q9UbZbdo7z0vXPbacLuxc1nPlvOkb_0fcHQxqU1Dn-i17fBUeV1Mcv_sjxhvXzvlVJq3qhJAyYD5YYDi0AgLLIU-o2XZ31DGFkvNQxioM8mPT5tOeycr-n4sfS=w518-h389-no\n",
        ")\n",
        "\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/JIjq0M7KAMEa8MkGResGW3R86x_CPJ1vskRyou_niy0ezQA-QH1dtbgHHZm8Jcu1uJhd6o-ZgVKgGQaYycGh-PWfCU-xgg6HGWRi_JtQiWPr764p1hsd76UWLiYtl3A1oJcTZKHiUU1q6l4LGpOYwRMnTMMT0fFqLjyq4EFZk3d16DYZeg541WcZMhqDLPJ1SpioL36phy_y-upXNzw464WhSHD5On-DBDbvpDWsxbE87Fhp6AXptRMrbHeOiZEmMsU6U9bdS0GWKJDJ1LEQuHb4d5hQevHFKUW-8RUnTC7Qu-Nx9WyleDaQYeNNmd-Gq79jMFcebQ4CwFwgUU3a19E7SrnecoBUqxCjNVJbOhowOmuVvHVgwULhQxKcH49IKRFjIJO0ZC80UNnjnq6drN7XrHPZDVhwV5cmyTTIMe0xUuxiGKBAI2VI0TZZkd7KxSeWIOx82qsTnbLRilYDUlqHZ43-H7DnJugCaHY9gi7uv0XnBMbaYTj58M-dcbLMEAhGjgro433DD1b9WJ7olKQOTpyzVO7OgTafUHV5kvdzA-o1KYFECaIh1EPDJu3FRDxYlLFC44yXGMQA7CXIcW2fKT02uviulyqk3Rfh-8LgqhzGSjQRX7pS7XCkcrL1AOikPCh9J3T_0c_caMri33b2J7Uezev6=w968-h703-no)\n",
        "\n",
        "y, the output layer, is the end:\n",
        "![alt text](https://lh3.googleusercontent.com/6J6pt5FDEceBFHqP18HZ3rdWE3oRHhOlFG2ZAeng6vROeQZuLajnYrCw2oAmPv39yYnDHi2R1K91kF1B9fyAZePNaTcZhyxOiW7U1MPeQUyBcMWH02wT4yrVyWIHn9Yz8vXlASd9G14mVgoxTvRvWtDBsQlVwf2gn48TLy3Xm7Z7JepB5X0yk_xusS5-7nwtspG8NkKQwVrJjCEzwR6wAN5KgUfFd15r1RI-6Hwb082hcipNWl5TX8pqvNdrMkrOjYHnNexc-CUGwIESRd3ramL72W4Wb1B8WciircrSFlt3bHRNO_11ZVB5W061dOhV8O9xhmPRiFQQImS99UpbK7Lwuu5FG4hLyr6rYPJsyMsMlT5yOsSjlypBTrKEJq8USJJmsPrGdbQ_hKtwuo-ywnmOTS4Bpz0eSI_JxNfzV4hFN4uo9LxcLqlSvUBH72HlDlPSsMpyMroieRJifYraW6XXpeZ__DXSbxWkm9oz4x-W1YJNQV_pljbHEEHTv4iGS22YyyLd84gPYoagm74ptwXodx2jJi9KHy2E_2HBvpVmQVgVgNFZq7-JO13PzzSOhhpT57EzBr1ToMbjRAaNl4KHCBjJY-5-l7mE1Q5-NTtMH5MV219sPi4za2Fq06hfDmJKMKDei1BgFxkczdmr92PdCm4-98z9=w518-h388-no)\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/S-ihGzoU2SJQvm36_2f0ORWDmJFe-gs2tjVB-MEZgKXAfhpLzwroAg7x7w7ynCDbycJtytdC5KmRTj0yNQ0mFBYLgE26L7CTl8HeZfjXnqOQkPqyR0tWRSmKg2VxscEEwmhEAu06kmI4YddkLXVIqx0cjPcuLqQSdOfprAUbH661aB5-H5XPdkiW83RCRGUYvUYZM0rpQhJM3LuV8jAO9ZEO3il87MQEahMKOPrsNp2KWsbDX9HF3B17_cz_ZyYRa7to30FdKOaOxL1R8ccFOzGHfEtfv5B5AuBX8Nqxq6LV-j1DSU_OTosxenEY238Bpc8aUnTLniN6T65YLQ1Re17yK28z3B-Mcc4OHsRYHObk2pZU7ASedCSO8SgW4pUyT1IJZpei0wNGLYRi50kLCgDraQr07aDodr-HpkP7jRAQcEFNWnFGPNsvA3sQz4QT2wZhEx9kVJLZdVHEsglCE1_bzdzKnE3MPv0QNAz72juBmicCxcNEv_p-BAlQsvkDrES1QdIn0PTXJAo8Wd2f1RndNWqdxlA4qBU1cwhNPjouv1CFbAQnDL9IiwbeXFD5AUjL4jDDUZYt8MGwZ1Mg6YGakLOWz-RZ05uitJFtQSV-UgC4tZ6K02kbLbv-8JYDK5-JoRfap6S6JeTapIa_nf_f1IFg397-=w968-h238-no)\n",
        "\n"
      ]
    }
  ]
}